# 机器学习四种调参方式

> 为了尽可能地让机器学习模型有更好地效果，在实际工作中对于超参数的优化必不可少，如何选用不同的调参策略会使得模型的训练和效果都大不相同。

将介绍4种常见的调参手法，它们分别是：

* 手动调参(类似于网格搜索)
* 网格搜索(GridSearch)
* 随机搜索(RandomSearch)
* 贝叶斯搜索(BayesSearch)

## 1 手动调参

思路：使用for循环和交叉验证的策略一步一步逼近最优参数的选取。

```python
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold,cross_val_score
from sklearn.datasets import load_wine

# 导入红酒数据集
wine = load_wine()
X = wine.data
y = wine.target

# 训练测试划分
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=14)

# 定义参数
k_value = list(range(2,11))  # KNN中近邻的数量
algorithm = ['auto','ball_tree','kd_tree','brute']  # KNN算法所使用的算法策略
scores = []  #交叉验证得分列表
best_comb = [] # 最优超参数组合列表
kfold = KFold(n_splits=5)  # K折检查验证(这里取5折)

# 开始调参for循环过程
# 首先从algorithm中遍历算法
# 再从knn的近邻中遍历k值
for algo in algorithm:
    for k in k_value:
        # 建立knn模型
        knn = KNeighborsClassifier(n_neighbors=k,algorithm=algo)
        # 使用训练集5折交叉的方式来进行交叉验证将结果储存再result中
        results = cross_val_score(knn,X_train,y_train,cv=kfold)
        # 打印结果均值的结果保留4位小数,并打印此时所使用的算法algo和近邻个数k
        print(f"score:{round(results.mean(),4)} with algo = {algo}, K={k}")
        # 将此时的分数均值append储存在列表内
        scores.append(results.mean())
        # 储存k值和算法algo的元组(k,algo)
        best_comb.append((k,algo))

# 打印最终结果
# 将最优参数组合赋给在best_param
best_param = best_comb[scores.index(max(scores))]
print(f"\nThe Best Score:{max(scores)}")
print(f"['algorithm':{best_param[1]},'n_neighbors':{best_param[0]}]")

# output
score:0.6697 with algo = auto, K=2
score:0.6773 with algo = auto, K=3
score:0.7177 with algo = auto, K=4
score:0.734 with algo = auto, K=5
score:0.7017 with algo = auto, K=6
score:0.7417 with algo = auto, K=7
score:0.7017 with algo = auto, K=8
score:0.6533 with algo = auto, K=9
score:0.6613 with algo = auto, K=10
score:0.6697 with algo = ball_tree, K=2
score:0.6773 with algo = ball_tree, K=3
score:0.7177 with algo = ball_tree, K=4
score:0.734 with algo = ball_tree, K=5
score:0.7017 with algo = ball_tree, K=6
score:0.7417 with algo = ball_tree, K=7
score:0.7017 with algo = ball_tree, K=8
score:0.6533 with algo = ball_tree, K=9
score:0.6613 with algo = ball_tree, K=10
score:0.6697 with algo = kd_tree, K=2
score:0.6773 with algo = kd_tree, K=3
score:0.7177 with algo = kd_tree, K=4
score:0.734 with algo = kd_tree, K=5
score:0.7017 with algo = kd_tree, K=6
score:0.7417 with algo = kd_tree, K=7
score:0.7017 with algo = kd_tree, K=8
score:0.6533 with algo = kd_tree, K=9
score:0.6613 with algo = kd_tree, K=10
score:0.6697 with algo = brute, K=2
score:0.6773 with algo = brute, K=3
score:0.7177 with algo = brute, K=4
score:0.734 with algo = brute, K=5
score:0.7017 with algo = brute, K=6
score:0.7417 with algo = brute, K=7
score:0.7017 with algo = brute, K=8
score:0.6533 with algo = brute, K=9
score:0.6613 with algo = brute, K=10

The Best Score:0.7416666666666667
['algorithm':auto,'n_neighbors':7]
```

手动调参易于理解，但缺点是：

* 不能保证得到最佳的参数组合。
* 这是一种反复试验的方法,因此会消耗更多的时间。

## 2 网格搜索

```python
# 导入网格搜索
from sklearn.model_selection import GridSearchCV
knn = KNeighborsClassifier()

# 设置网格搜索所需的参数组合
grid_param = {'n_neighbors':list(range(2,11),
              'algorithm':['auto','ball_tree','kd_tree','brute']}

# 使用模型和参数组合拟合训练集
grid = GridSearchCV(estimator=knn,param_grid=grid_param,cv=5)
grid.fit(X_train,y_train)

# best parameter combination
print("最优参数:",grid.best_params_)

# Score achieved with best parameter combination
print("最优参数的最优分数:",grid.best_score_)

# all combinations of hyperparameters
print("所有的参数组合",grid.cv_results_['params'])

# average scores of cross-validation
print("平均测试分数:",grid.cv_results_['mean_test_score'])

# output
最优参数: {'algorithm': 'auto', 'n_neighbors': 5}
最优参数的最优分数: 0.774
所有的参数组合 [{'algorithm': 'auto', 'n_neighbors': 2}, 
{'algorithm': 'auto', 'n_neighbors': 3}, 
{'algorithm': 'auto', 'n_neighbors': 4}, 
{'algorithm': 'auto', 'n_neighbors': 5}, 
{'algorithm': 'auto', 'n_neighbors': 6}, 
{'algorithm': 'auto', 'n_neighbors': 7}, 
{'algorithm': 'auto', 'n_neighbors': 8}, 
{'algorithm': 'auto', 'n_neighbors': 9}, 
{'algorithm': 'auto', 'n_neighbors': 10}, 
{'algorithm': 'ball_tree', 'n_neighbors': 2}, 
{'algorithm': 'ball_tree', 'n_neighbors': 3}, 
{'algorithm': 'ball_tree', 'n_neighbors': 4}, 
{'algorithm': 'ball_tree', 'n_neighbors': 5}, 
{'algorithm': 'ball_tree', 'n_neighbors': 6}, 
{'algorithm': 'ball_tree', 'n_neighbors': 7}, 
{'algorithm': 'ball_tree', 'n_neighbors': 8}, 
{'algorithm': 'ball_tree', 'n_neighbors': 9}, 
{'algorithm': 'ball_tree', 'n_neighbors': 10}, 
{'algorithm': 'kd_tree', 'n_neighbors': 2}, 
{'algorithm': 'kd_tree', 'n_neighbors': 3}, 
{'algorithm': 'kd_tree', 'n_neighbors': 4}, 
{'algorithm': 'kd_tree', 'n_neighbors': 5}, 
{'algorithm': 'kd_tree', 'n_neighbors': 6}, 
{'algorithm': 'kd_tree', 'n_neighbors': 7}, 
{'algorithm': 'kd_tree', 'n_neighbors': 8}, 
{'algorithm': 'kd_tree', 'n_neighbors': 9}, 
{'algorithm': 'kd_tree', 'n_neighbors': 10}, 
{'algorithm': 'brute', 'n_neighbors': 2}, 
{'algorithm': 'brute', 'n_neighbors': 3}, 
{'algorithm': 'brute', 'n_neighbors': 4}, 
{'algorithm': 'brute', 'n_neighbors': 5}, 
{'algorithm': 'brute', 'n_neighbors': 6}, 
{'algorithm': 'brute', 'n_neighbors': 7}, 
{'algorithm': 'brute', 'n_neighbors': 8}, 
{'algorithm': 'brute', 'n_neighbors': 9}, 
{'algorithm': 'brute', 'n_neighbors': 10}]
平均测试分数: [0.66966667 0.66933333 0.70966667 0.774      0.70966667 0.72566667
 0.71766667 0.66933333 0.66133333 0.66966667 0.66933333 0.70966667
 0.774      0.70966667 0.72566667 0.71766667 0.66933333 0.66133333
 0.66966667 0.66933333 0.70966667 0.774      0.70966667 0.72566667
 0.71766667 0.66933333 0.66133333 0.66966667 0.66933333 0.70966667
 0.774      0.70966667 0.72566667 0.71766667 0.66933333 0.66133333]
```

网格搜索的缺点：由于它尝试每一种超参数组合,并根据交叉验证分数选择最佳组合,这使得GridSearchCV极其缓慢。

## 3 随机搜索

```python
from sklearn.model_selection import RandomizedSearchCV
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()

# 定义随机搜索的参数组合
grid_param = {'n_neighbors':list(range(2,11)),
              'algorithm':['auto','ball_tree','kd_tree','brute']}

# 实例化随机搜索器
# 随机搜索可以设置迭代次数n_iter
rand_ser = RandomizedSearchCV(estimator=knn,param_distributions=grid_param,n_iter=10,)
rand_ser.fit(X_train,y_train)

# best parameter combination
print("最优参数:",rand_ser.best_params_)

# score achieved with the best parameter combination
print("最优分数:",rand_ser.best_score_)

# all combinations of hyperparameters
print("交叉验证的参数组合:",rand_ser.cv_results_['params'])

# average scores of cross-validation
print("交叉验证平均测试集误差:",rand_ser.cv_results_['mean_test_score'])

# output
最优参数: {'n_neighbors': 7, 'algorithm': 'ball_tree'}
最优分数: 0.7256666666666667
交叉验证的参数组合: [{'n_neighbors': 2, 'algorithm': 'kd_tree'}, 
{'n_neighbors': 2, 'algorithm': 'brute'}, 
{'n_neighbors': 8, 'algorithm': 'brute'}, 
{'n_neighbors': 7, 'algorithm': 'ball_tree'}, 
{'n_neighbors': 10, 'algorithm': 'brute'}, 
{'n_neighbors': 10, 'algorithm': 'ball_tree'}, 
{'n_neighbors': 4, 'algorithm': 'ball_tree'}, 
{'n_neighbors': 8, 'algorithm': 'auto'}, 
{'n_neighbors': 6, 'algorithm': 'auto'}, 
{'n_neighbors': 6, 'algorithm': 'brute'}]
交叉验证平均测试集误差: [0.66966667 0.66966667 0.71766667 0.72566667 0.66133333 0.66133333
 0.70966667 0.71766667 0.70966667 0.70966667]
```

## 4 贝叶斯优化

```python
# 从skopt包中导入BayesSearchCV搜索器
# 安装scikit-optimize
# pip install scikit-optimize

from skopt import BayesSearchCV
import warnings
warnings.filterwarnings("ignore")

# parameter ranges are specified by one of below
# from skopt.space import Real, Categorical, Integer

knn = KNeighborsClassifier()

# 定义贝叶斯搜索的参数组合
grid_param = {'n_neighbors':list(range(2,11)),
              'algorithm':['auto','ball_tree','kd_tree','brute']}

# 实例化贝叶斯搜索器,传入分类模型和参数组合以及迭代次数
Bayes = BayesSearchCV(knn,grid_param,n_iter=10,random_state=14)
Bayes.fit(X_train,y_train)

# best parameter combination
print("最优参数:",Bayes.best_params_)

# Score achieved with best parameter combination
print("最优分数:",Bayes.best_score_)

# all combinations of hyperparameter
print("交叉验证参数组合:",Bayes.cv_results_['params'])

# average scores of cross-validation
print("交叉验证测试分数:",Bayes.cv_results_['mean_test_score'])

# output
最优参数: OrderedDict([('algorithm', 'ball_tree'), ('n_neighbors', 5)])
最优分数: 0.7741935483870968
交叉验证参数组合: 
[OrderedDict([('algorithm', 'brute'), ('n_neighbors', 2)]), 
OrderedDict([('algorithm', 'brute'), ('n_neighbors', 7)]), 
OrderedDict([('algorithm', 'kd_tree'), ('n_neighbors', 3)]), 
OrderedDict([('algorithm', 'auto'), ('n_neighbors', 2)]), 
OrderedDict([('algorithm', 'brute'), ('n_neighbors', 3)]), 
OrderedDict([('algorithm', 'brute'), ('n_neighbors', 7)]), 
OrderedDict([('algorithm', 'ball_tree'), ('n_neighbors', 5)]), 
OrderedDict([('algorithm', 'kd_tree'), ('n_neighbors', 8)]), 
OrderedDict([('algorithm', 'brute'), ('n_neighbors', 5)]), 
OrderedDict([('algorithm', 'brute'), ('n_neighbors', 8)])]
交叉验证测试分数: [0.6693548387096774, 0.7258064516129032, 0.6693548387096774, 
0.6693548387096774, 0.6693548387096774, 0.7258064516129032, 0.7741935483870968, 
0.717741935483871, 0.7741935483870968, 0.717741935483871]
```

值得一提的是，另一个可以实现贝叶斯搜索的类似库是bayesian-optimization

可使用以下指令来安装

```python
!pip install bayesian-optimization
```

## 参考资料

[1] [https://**thuijskens.github.io/20**16/12/29/bayesian-optimisation/](https://link.zhihu.com/?target=https%3A//thuijskens.github.io/2016/12/29/bayesian-optimisation/)

[2] [https://**scikit-optimize.github.io**/stable/modules/generated/skopt.BayesSearchCV.html](https://link.zhihu.com/?target=https%3A//scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html)

[3] [https://**github.com/fmfn/Bayesia**nOptimization](https://link.zhihu.com/?target=https%3A//github.com/fmfn/BayesianOptimization)

[4] [4种主流超参数调优技术](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/KzCoi3vngE5vOHa-PpvB9g)
