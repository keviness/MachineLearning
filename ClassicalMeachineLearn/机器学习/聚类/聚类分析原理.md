# 层次聚类（Python代码）

本篇想和大家介绍下 **[层次聚类](https://www.zhihu.com/search?q=%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A435987610%7D)** ，先通过一个简单的例子介绍它的基本理论，然后再用一个实战案例 `Python`代码实现聚类效果。

首先要说，聚类属于[机器学习](https://www.zhihu.com/search?q=%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A435987610%7D)的无监督学习，而且也分很多种方法，比如大家熟知的有 `K-means`。层次聚类也是聚类中的一种，也很常用。下面我先简单回顾一下 `K-means`的基本原理，然后慢慢引出层次聚类的定义和分层步骤，这样更有助于大家理解。

## 层次聚类和K-means有什么不同？

`K-means` 工作原理可以简要概述为：

* 决定簇数（k）
* 从数据中随机选取 k 个点作为[质心](https://www.zhihu.com/search?q=%E8%B4%A8%E5%BF%83&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A435987610%7D)
* 将所有点分配到最近的[聚类质心](https://www.zhihu.com/search?q=%E8%81%9A%E7%B1%BB%E8%B4%A8%E5%BF%83&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A435987610%7D)
* 计算新形成的簇的质心
* 重复步骤 3 和 4

这是一个迭代过程，直到新形成的簇的质心不变，或者达到最大迭代次数。

但是 `K-means` 是存在一些缺点的，我们必须在算法开始前就决定簇数 `K` 的数量，但实际我们并不知道应该有多少个簇，所以一般都是根据自己的理解先设定一个值，这就可能导致我们的理解和实际情况存在一些偏差。

层次聚类完全不同，它不需要我们开始的时候指定簇数，而是先完整的形成整个层次聚类后，通过决定合适的距离，自动就可以找到对应的簇数和聚类。

## 什么是层次聚类？

下面我们由浅及深的介绍什么是层次聚类，先来一个简单的例子。

假设我们有以下几点，我们想将它们[分组](https://www.zhihu.com/search?q=%E5%88%86%E7%BB%84&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A435987610%7D)：

![](https://pic1.zhimg.com/80/v2-2c443af0b46672e61209b40deafca454_1440w.jpg)

我们可以将这些点中的每一个分配给一个单独的簇，就是4个簇（4种颜色）：

![](https://pic2.zhimg.com/80/v2-c32bed9dcd1806a3d3cdbd7752cd506d_1440w.jpg)

然后基于这些簇的相似性（距离），将最相似的（距离最近的）点组合在一起并重复这个过程，直到只剩下一个[集群](https://www.zhihu.com/search?q=%E9%9B%86%E7%BE%A4&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A435987610%7D)：

![](https://pic4.zhimg.com/80/v2-b1edfc285bcab3d2b48717a31c19dde7_1440w.jpg)

上面本质上就是在构建一个层次结构。先了解到这里，后面我们详细介绍它的分层步骤。

## 层次聚类的类型

主要有两种类型的层次聚类：

* 凝聚层次聚类
* 分裂层次聚类

### 凝聚层次聚类

先让所有点分别成为一个单独的簇，然后通过[相似性](https://www.zhihu.com/search?q=%E7%9B%B8%E4%BC%BC%E6%80%A7&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A435987610%7D)不断组合，直到最后只有一个簇为止，这就是凝聚层次聚类的过程，和我们上面刚刚说的一致。

### 分裂层次聚类

分裂层次聚类正好反过来，它是从单个集群开始逐步分裂，直到无法分裂，即每个点都是一个簇。

所以无论是 10、100、1000 个数据点都不重要，这些点在开始的时候都属于同一个簇：

![](https://pic3.zhimg.com/80/v2-fee2d09c1714150abd0eafceb202775a_1440w.jpg)

现在，在每次迭代中拆分簇中相隔最远的两点，并重复这个过程，直到每个簇只包含一个点：

![](https://pic2.zhimg.com/80/v2-c32bed9dcd1806a3d3cdbd7752cd506d_1440w.jpg)

上面的过程就是 **分裂层次聚类** 。

## 执行层次聚类的步骤

上面已经说了层次聚类的大概过程，那关键的来了，如何确定点和点的相似性呢？

这是聚类中最重要的问题之一了，一般计算相似度的方法是： **计算这些簇的质心之间的距离** 。距离最小的点称为相似点，我们可以合并它们，也可以将其称为 **基于距离的算法** 。

另外在层次聚类中，还有一个称为**邻近矩阵**的概念，它存储了每个点之间的距离。下面我们通过一个例子来理解如何计算相似度、邻近矩阵、以及层次聚类的具体步骤。

### 案例介绍

假设一位老师想要将学生分成不同的组。现在有每个学生在作业中的分数，想根据这些分数将他们分成几组。关于拥有多少组，这里没有固定的目标。由于老师不知道应该将哪种类型的学生分配到哪个组，因此不能作为监督学习问题来解决。下面，我们将尝试应用层次聚类将学生分成不同的组。

下面是个5名学生的成绩：

![](https://pic4.zhimg.com/80/v2-ac78eced09294331601640e1f07c4197_1440w.jpg)

### 创建邻近矩阵

首先，我们要创建一个邻近矩阵，它储存了每个点两两之间的距离，因此可以得到一个形状为 n X n 的方阵。

这个案例中，可以得到以下 5 x 5 的邻近矩阵：

![](https://pic3.zhimg.com/80/v2-c2d87caff46c2657df0e206299fccc5e_1440w.jpg)

矩阵里有两点需要注意下：

* 矩阵的对角元素始终为 0，因为点与其自身的距离始终为 0
* 使用[欧几里得距离公式](https://www.zhihu.com/search?q=%E6%AC%A7%E5%87%A0%E9%87%8C%E5%BE%97%E8%B7%9D%E7%A6%BB%E5%85%AC%E5%BC%8F&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A435987610%7D)来计算非对角元素的距离

比如，我们要计算点 1 和 2 之间的距离，计算公式为：

![[公式]](https://www.zhihu.com/equation?tex=%5Csqrt%7B%2810-7%29%5E2%7D%3D%5Csqrt%7B9%7D%3D3+)

同理，按此计算方法完成后填充邻近矩阵其余元素。

### 执行层次聚类

这里使用凝聚层次聚类来实现。

步骤 1： 首先，我们将所有点分配成单个簇：

![](https://pic2.zhimg.com/80/v2-bf89c4f5465ad31b635029d6b68c5c19_1440w.png)

这里不同的颜色代表不同的簇，我们数据中的 5 个点，即有 5 个不同的簇。

步骤2： 接下来，我们需要 **查找邻近矩阵中的最小距离并合并距离最小的点** 。然后我们更新邻近矩阵：

![](https://pic4.zhimg.com/80/v2-13cf65f9eedfd52756a08d35ca914567_1440w.jpg)

最小距离是 3，因此我们将合并点 1 和 2：

![](https://pic2.zhimg.com/80/v2-633c1a00570b866295329146af03c689_1440w.jpg)

让我们看看更新的集群并相应地更新邻近矩阵：

![](https://pic2.zhimg.com/80/v2-53d3a2f6a2873de16b7a3833f51a4f5d_1440w.jpg)

更新之后，我们取了1、2 两个点中值 (7, 10) 最大的来替换这个簇的值。当然除了最大值之外，我们还可以取最小值或[平均值](https://www.zhihu.com/search?q=%E5%B9%B3%E5%9D%87%E5%80%BC&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A435987610%7D)。然后，我们将再次计算这些簇的邻近矩阵：

![](https://pic3.zhimg.com/80/v2-759b8ab87be7a0a91a7c47fb698c7eee_1440w.jpg)

第 3 步： 重复第 2 步，直到只剩下一个簇。

重复所有的步骤后，我们将得到如下所示的合并的聚类：

![](https://pic3.zhimg.com/80/v2-d4401afec2f64313ab46cb437e304d3e_1440w.jpg)

这就是凝聚层次聚类的工作原理。但问题是我们仍然不知道该分几组？是2、3、还是4组呢？

下面开始介绍如何选择聚类数。

## 如何选择聚类数？

为了获得层次聚类的簇数，我们使用了一个概念，叫作 **[树状图](https://www.zhihu.com/search?q=%E6%A0%91%E7%8A%B6%E5%9B%BE&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A435987610%7D)** 。

通过树状图，我们可以更方便的选出聚类的簇数。

回到上面的例子。当我们合并两个簇时，树状图会相应地记录这些簇之间的距离并以图形形式表示。下面这个是树状图的原始状态，横坐标记录了每个点的标记，[纵轴](https://www.zhihu.com/search?q=%E7%BA%B5%E8%BD%B4&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A435987610%7D)记录了点和点之间的距离：

![](https://pic3.zhimg.com/80/v2-10abc9ad48653d28374980f948c20cfa_1440w.jpg)

当合并两个簇时，将会在树状图中连接起来，连接的高度就是点之间的距离。下面是我们刚刚层次聚类的过程。

![](https://pic3.zhimg.com/80/v2-d4401afec2f64313ab46cb437e304d3e_1440w.jpg)

然后开始对上面的过程进行树状图的绘制。从合并样本 1 和 2 开始，这两个样本之间的距离为 3。

![](https://pic3.zhimg.com/80/v2-5eac9c861e86fe11076df0e653cbea46_1440w.jpg)

可以看到已经合并了 1 和 2。垂直线代表 1 和 2 的距离。同理，按照层次聚类过程绘制合并簇类的所有步骤，最后得到了这样的树状图：

![](https://pic4.zhimg.com/80/v2-9d67291c05d0d95137a684d2f6524cbb_1440w.jpg)

通过树状图，我们可以清楚地形象化层次聚类的步骤。树状图中垂直线的距离越远代表簇之间的距离越大。

有了这个树状图，我们决定簇类数就方便多了。

现在我们可以设置一个 **阈值距离** ，绘制一条[水平线](https://www.zhihu.com/search?q=%E6%B0%B4%E5%B9%B3%E7%BA%BF&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A435987610%7D)。比如我们将阈值设置为 12，并绘制一条水平线，如下：

![](https://pic2.zhimg.com/80/v2-08a59f12cc81e804fa9a1cfa3bf2565d_1440w.jpg)

从交点中可以看到，聚类的数量就是与阈值水平线与垂直线相交的数量（红线与 2 条垂直线相交，我们将有 2 个簇）。与横坐标相对应的，一个簇将有一个样本集合为 (1,2,4)，另一个集群将有一个样本集合 (3,5)。

这样，我们就通过树状图解决了分层聚类中要决定聚类的数量。

## Python代码实战案例

上面是理论基础，有点数学基础都能看懂。下面介绍下在如何用代码 `Python`来实现这一过程。这里拿一个**客户细分**的数据来展示一下。

数据集和代码在我的GitHub里：

> [https://**github.com/xiaoyusmd/Py**thonDataScience](https://link.zhihu.com/?target=https%3A//github.com/xiaoyusmd/PythonDataScience)

**分享不易，还请给个star！**

这个数据来源于UCI 机器学习库。我们的目的是根据批发分销商的客户在不同产品类别（如牛奶、杂货、地区等）上的年度支出，对他们进行细分。

首先对数据进行一个标准化，为了让所有数据在同一个维度便于计算，然后应用层次聚类来细分客户。

```python
from sklearn.preprocessing import normalize
data_scaled = normalize(data)
data_scaled = pd.DataFrame(data_scaled, columns=data.columns)

import scipy.cluster.hierarchy as shc
plt.figure(figsize=(10, 7))  
plt.title("Dendrograms")  
dend = shc.dendrogram(shc.linkage(data_scaled, method='ward'))
```

![](https://pic3.zhimg.com/80/v2-08475b87f7cdb85a35d2a9c8227e642e_1440w.jpg)

x 轴包含了所有样本，y 轴代表这些样本之间的距离。距离最大的垂直线是蓝线，假如我们决定要以阈值 6 切割树状图：

```python3
plt.figure(figsize=(10, 7))  
plt.title("Dendrograms")  
dend = shc.dendrogram(shc.linkage(data_scaled, method='ward'))
plt.axhline(y=6, color='r', linestyle='--')
```

![](https://pic4.zhimg.com/80/v2-47bf730984842610d081be8a34893e27_1440w.jpg)

现在我们有两个簇了，我们要对这 2 个簇应用层次聚类：

```python
from sklearn.cluster import AgglomerativeClustering
cluster = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')  
cluster.fit_predict(data_scaled)
```

![](https://pic4.zhimg.com/80/v2-95a39a6e3a98f4e5a2cba2cec602afc3_1440w.jpg)

由于我们定义了 2 个簇，因此我们可以在输出中看到 0 和 1 的值。0 代表属于第一个簇的点，1 代表属于第二个簇的点。

```python3
plt.figure(figsize=(10, 7))  
plt.scatter(data_scaled['Milk'], data_scaled['Grocery'], c=cluster.labels_)
```

![](https://pic4.zhimg.com/80/v2-811fe2d37caf9dcbbeb9faab7b93babb_1440w.jpg)到这里我们就成功的完成了聚类。

> 参考：[https://www.**analyticsvidhya.com/blo**](https://link.zhihu.com/?target=https%3A//www.analyticsvidhya.com/blog/2019/05/beginners-guide-hierarchical-clustering/)
