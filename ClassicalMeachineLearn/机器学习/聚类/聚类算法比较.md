# 常用聚类算法

## **一、什么是聚类**

## **1.1 聚类的定义**

`聚类(Clustering)`是按照某个特定标准(如距离)把一个数据集分割成不同的类或簇，使得 **同一个簇内的数据对象的相似性尽可能大，同时不在同一个簇中的数据对象的差异性也尽可能地大** 。也即聚类后同一类的数据尽可能聚集到一起，不同类数据尽量分离。

## **1.2 聚类和分类的区别**

* `聚类(Clustering)`：是指把相似的数据划分到一起，具体划分的时候并不关心这一类的标签，目标就是把相似的数据聚合到一起，聚类是一种 `无监督学习(Unsupervised Learning)`方法。
* `分类(Classification)`：是把不同的数据划分开，其过程是通过训练数据集获得一个分类器，再通过分类器去预测未知数据，分类是一种 `监督学习(Supervised Learning)`方法。

## **1.3 聚类的一般过程**

1. 数据准备：特征标准化和降维
2. 特征选择：从最初的特征中选择最有效的特征，并将其存储在向量中
3. 特征提取：通过对选择的特征进行转换形成新的突出特征
4. 聚类：基于某种距离函数进行相似度度量，获取簇
5. 聚类结果评估：分析聚类结果，如 `距离误差和(SSE)`等

## **1.4 数据对象间的相似度度量**

对于数值型数据，可以使用下表中的相似度度量方法。

![](https://pic3.zhimg.com/80/v2-921d5ff0b665e67d5df54047b9d6531a_1440w.jpg)

`Minkowski`距离就是![[公式]](https://www.zhihu.com/equation?tex=+Lp+)范数（![[公式]](https://www.zhihu.com/equation?tex=p%E2%89%A51))，而 `Manhattan` 距离、`Euclidean`距离、`Chebyshev`距离分别对应 ![[公式]](https://www.zhihu.com/equation?tex=p%3D1%2C2%2C%E2%88%9E+)时的情形。

## **1.5 cluster之间的相似度度量**

除了需要衡量对象之间的距离之外，有些聚类算法（如层次聚类）还需要衡量 `cluster`之间的距离 ，假设![[公式]](https://www.zhihu.com/equation?tex=+C_i+)和![[公式]](https://www.zhihu.com/equation?tex=+C_j) 为两个 `cluster`，则前四种方法定义的 ![[公式]](https://www.zhihu.com/equation?tex=C_i+)和 ![[公式]](https://www.zhihu.com/equation?tex=C_j) 之间的距离如下表所示。

![](https://pic4.zhimg.com/80/v2-efc243a0fd595089412bd617eaa0d78b_1440w.jpg)

* `Single-link`定义两个 `cluster`之间的距离为两个 `cluster`之间距离最近的两个点之间的距离，这种方法会在聚类的过程中产生 `链式效应`，即有可能会出现非常大的 `cluster`
* `Complete-link`定义的是两个 `cluster`之间的距离为两个``cluster `之间距离最远的两个点之间的距离，这种方法可以避免`链式效应`,对异常样本点（不符合数据集的整体分布的噪声点）却非常敏感，容易产生不合理的聚类
* `UPGMA`正好是 `Single-link`和 `Complete-link`方法的折中，他定义两个 `cluster`之间的距离为两个 `cluster`之间所有点距离的平均值
* 最后一种 `WPGMA`方法计算的是两个 `cluster` 之间两个对象之间的距离的加权平均值，加权的目的是为了使两个 `cluster` 对距离的计算的影响在同一层次上，而不受 `cluster` 大小的影响，具体公式和采用的权重方案有关。

## **二、数据聚类方法**

数据聚类方法主要可以分为 `划分式聚类方法(Partition-based Methods)`、`基于密度的聚类方法(Density-based methods)`、`层次化聚类方法(Hierarchical Methods)`等。

![](https://pic4.zhimg.com/80/v2-05343ed3ff48cec83244ab64760acdeb_1440w.jpg)

## **2.1 划分式聚类方法**

划分式聚类方法需要事先指定簇类的数目或者聚类中心，通过反复迭代，直至最后达到"簇内的点足够近，簇间的点足够远"的目标。经典的划分式聚类方法有 `k-means`及其变体 `k-means++`、`bi-kmeans`、`kernel k-means`等。

### **2.1.2 k-means算法**

经典的 `k-means`算法的流程如下：

![](https://pic2.zhimg.com/80/v2-9a7fb4c3e6c18143890de949a309be21_1440w.jpg)

经典 `k-means` **[源代码](https://link.zhihu.com/?target=https%3A//github.com/HuStanding/nlp-exercise/blob/master/cluster/kmeans/kmeans.py)** ，下左图是原始数据集，通过观察发现大致可以分为4类，所以取![[公式]](https://www.zhihu.com/equation?tex=k%3D4)，测试数据效果如下右图所示。

![](https://pic4.zhimg.com/80/v2-ed9693a3d8d82d6ebb459949ec475877_1440w.jpg)

![](https://pic4.zhimg.com/v2-f41b46d12b9d89937864cfb7b99d498b_b.jpg)

看起来很顺利，但事情并非如此，我们考虑 `k-means`算法中最核心的部分，假设![[公式]](https://www.zhihu.com/equation?tex=x_i%28i%3D1%2C2%2C%E2%80%A6%2Cn%29)是数据点，![[公式]](https://www.zhihu.com/equation?tex=%5Cmu_j%28j%3D1%2C2%2C%E2%80%A6%2Ck%29)是初始化的数据中心，那么我们的目标函数可以写成

![[公式]](https://www.zhihu.com/equation?tex=%5Cmin%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+%5Cmin+%5Climits_%7Bj%3D1%2C2%2C...%2Ck%7D%5Cleft+%7C%5Cleft+%7C++x_i+-%5Cmu_j%5Cright+%7C+%5Cright+%7C%5E2+%5C%5C)

这个函数是 **非凸优化函数** ，会收敛于局部最优解，可以参考 **[证明过程](https://link.zhihu.com/?target=https%3A//math.stackexchange.com/questions/463453/how-to-see-that-k-means-objective-is-convex)** 。举个 ，![[公式]](https://www.zhihu.com/equation?tex=%5Cmu_1%3D%5Cleft+%5B+1%2C1%5Cright+%5D+%2C%5Cmu_2%3D%5Cleft+%5B+-1%2C-1%5Cright+%5D)，则

![[公式]](https://www.zhihu.com/equation?tex=z%3D%5Cmin+%5Climits_%7Bj%3D1%2C2%7D%5Cleft+%7C%5Cleft+%7C++x_i+-%5Cmu_j%5Cright+%7C+%5Cright+%7C%5E2+%5C%5C)

该函数的曲线如下图所示

![](https://pic1.zhimg.com/80/v2-7546836911d5c6cd10a663ac4f04fd1c_1440w.jpg)

可以发现该函数有两个局部最优点，当时初始质心点取值不同的时候，最终的聚类效果也不一样，接下来我们看一个具体的实例。

![](https://pic1.zhimg.com/80/v2-05589152e1293cb4193af61926eb478c_1440w.jpg)

在这个例子当中，下方的数据应该归为一类，而上方的数据应该归为两类，这是由于初始质心点选取的不合理造成的误分。而![[公式]](https://www.zhihu.com/equation?tex=k)值的选取对结果的影响也非常大，同样取上图中数据集，取![[公式]](https://www.zhihu.com/equation?tex=k%3D2%2C3%2C4)，可以得到下面的聚类结果：

![](https://pic2.zhimg.com/80/v2-18a21f32dcad2b997cb25ade06534451_1440w.jpg)

一般来说，经典 `k-means`算法有以下几个特点：

1. 需要提前确定![[公式]](https://www.zhihu.com/equation?tex=k)值
2. 对初始质心点敏感
3. 对异常数据敏感

### **2.1.2 k-means++算法**

`k-means++`是针对 `k-means`中初始质心点选取的优化算法。该算法的流程和 `k-means`类似，改变的地方只有初始质心的选取，该部分的算法流程如下

![](https://pic2.zhimg.com/80/v2-62cfa5ed9ec45b638be17185375f3d4d_1440w.jpg)

`k-means++` **[源代码](https://link.zhihu.com/?target=https%3A//github.com/HuStanding/nlp-exercise/blob/master/cluster/kmeans/kmeans%252B%252B.py)** ，使用 `k-means++`对上述数据做聚类处理，得到的结果如下

![](https://pic2.zhimg.com/80/v2-d8f5868de9d99e980d157cd634564979_1440w.jpg)

### **2.1.3 bi-kmeans算法**

一种度量聚类效果的指标是 `SSE(Sum of Squared Error)`，他表示聚类后的簇离该簇的聚类中心的平方和，`SSE`越小，表示聚类效果越好。 `bi-kmeans`是针对 `kmeans`算法会陷入局部最优的缺陷进行的改进算法。该算法基于SSE最小化的原理，首先将所有的数据点视为一个簇，然后将该簇一分为二，之后选择其中一个簇继续进行划分，选择哪一个簇进行划分取决于对其划分是否能最大程度的降低 `SSE`的值。

该算法的流程如下：

![](https://pic2.zhimg.com/80/v2-213acdea55011ee7382bb327a75c3c59_1440w.jpg)

`bi-kmeans`算法 **[源代码](https://link.zhihu.com/?target=https%3A//github.com/HuStanding/nlp-exercise/blob/master/cluster/kmeans/bi_kmeans.py)** ，利用 `bi-kmeans`算法处理上节中的数据得到的结果如下图所示。

![](https://pic4.zhimg.com/80/v2-e1e127fc6ccca98b9d722c0be2f428f3_1440w.jpg)

这是一个全局最优的方法，所以每次计算出来的 `SSE`值基本也是一样的（ **但是还是不排除有部分随机分错的情况** ），我们和前面的 `k-means`、`k-means++`比较一下计算出来的 `SSE`值

![](https://pic3.zhimg.com/80/v2-5e60c9e9d1102da5d33ff1bb230f1932_1440w.jpg)

可以看到，`k-means`每次计算出来的 `SSE`都较大且不太稳定，`k-means++`计算出来的 `SSE`较稳定并且数值较小，而 `bi-kmeans` 4次计算出来的 `SSE`都一样，并且计算的 `SSE`都较小，说明聚类的效果也最好。

## **2.2 基于密度的方法**

`k-means`算法对于凸性数据具有良好的效果，能够根据距离来讲数据分为球状类的簇，但对于非凸形状的数据点，就无能为力了，当 `k-means`算法在环形数据的聚类时，我们看看会发生什么情况。

![](https://pic4.zhimg.com/80/v2-014e1a2055df95e067341cba1809a9af_1440w.jpg)

从上图可以看到，`kmeans`聚类产生了错误的结果，这个时候就需要用到基于密度的聚类方法了，该方法需要定义两个参数![[公式]](https://www.zhihu.com/equation?tex=%5Cvarepsilon)和![[公式]](https://www.zhihu.com/equation?tex=M)，分别表示密度的邻域半径和邻域密度阈值。`DBSCAN`就是其中的典型。

### **2.2.1 DBSCAN算法**

首先介绍几个概念，考虑集合![[公式]](https://www.zhihu.com/equation?tex=X%3D%5Cleft+%5C%7Bx%5E%7B%281%29%7D%2Cx%5E%7B%282%29%7D%2C...%2Cx%5E%7B%28n%29%7D%5Cright+%5C%7D)，![[公式]](https://www.zhihu.com/equation?tex=%5Cvarepsilon)表示定义密度的邻域半径，设聚类的邻域密度阈值为![[公式]](https://www.zhihu.com/equation?tex=M)，有以下定义：

* **![[公式]](https://www.zhihu.com/equation?tex=%5Cvarepsilon)邻域(![[公式]](https://www.zhihu.com/equation?tex=%5Cvarepsilon)-neighborhood）**

![[公式]](https://www.zhihu.com/equation?tex=N_%7B%5Cvarepsilon+%7D%28x%29%3D%5Cleft+%5C%7By%5Cin++X%7Cd%28x%2C+y%29+%3C+%5Cvarepsilon+%5Cright+%5C%7D+%5C%5C)

* **密度(desity)**![[公式]](https://www.zhihu.com/equation?tex=x)的密度为

![[公式]](https://www.zhihu.com/equation?tex=%5Crho+%28x%29%3D%5Cleft+%7C+N_%7B%5Cvarepsilon+%7D%28x%29%5Cright+%7C+%5C%5C)

* **核心点(core-point)**

设![[公式]](https://www.zhihu.com/equation?tex=x%5Cin++X)，若![[公式]](https://www.zhihu.com/equation?tex=%5Crho+%28x%29+%5Cgeq+M)，则称![[公式]](https://www.zhihu.com/equation?tex=x)为![[公式]](https://www.zhihu.com/equation?tex=X)的核心点，记![[公式]](https://www.zhihu.com/equation?tex=X)中所有核心点构成的集合为![[公式]](https://www.zhihu.com/equation?tex=X_c)，记所有非核心点构成的集合为![[公式]](https://www.zhihu.com/equation?tex=X_%7Bnc%7D)。

* **边界点(border-point)**

若![[公式]](https://www.zhihu.com/equation?tex=x%5Cin++X_%7Bnc%7D)，且![[公式]](https://www.zhihu.com/equation?tex=%5Cexists+y%5Cin++X)，满足

![[公式]](https://www.zhihu.com/equation?tex=y%5Cin++N_%7B%5Cvarepsilon+%7D%28x%29+%5Ccap+X_c+%5C%5C)

即![[公式]](https://www.zhihu.com/equation?tex=x)的![[公式]](https://www.zhihu.com/equation?tex=%5Cvarepsilon)邻域中存在核心点，则称![[公式]](https://www.zhihu.com/equation?tex=x)为![[公式]](https://www.zhihu.com/equation?tex=X)的边界点，记![[公式]](https://www.zhihu.com/equation?tex=X)中所有的边界点构成的集合为![[公式]](https://www.zhihu.com/equation?tex=X_%7Bbd%7D)。

此外，边界点也可以这么定义：若![[公式]](https://www.zhihu.com/equation?tex=x%5Cin++X_%7Bnc%7D)，且![[公式]](https://www.zhihu.com/equation?tex=x)落在某个核心点的![[公式]](https://www.zhihu.com/equation?tex=%5Cvarepsilon)邻域内，则称![[公式]](https://www.zhihu.com/equation?tex=x)为![[公式]](https://www.zhihu.com/equation?tex=X)的一个边界点，一个边界点可能同时落入一个或多个核心点的![[公式]](https://www.zhihu.com/equation?tex=%5Cvarepsilon)邻域。

* **噪声点(noise-point)**

若![[公式]](https://www.zhihu.com/equation?tex=x)满足

![[公式]](https://www.zhihu.com/equation?tex=x%5Cin++X%2Cx+%5Cnotin+X_%7Bc%7D%E4%B8%94+x%5Cnotin+X_%7Bbd%7D+%5C%5C)

则称![[公式]](https://www.zhihu.com/equation?tex=x)为噪声点。

如下图所示，设![[公式]](https://www.zhihu.com/equation?tex=M%3D3)，则A为核心点，B、C是边界点，而N是噪声点。

![](https://pic1.zhimg.com/80/v2-7efff68d8eada2472ed0a0372fa0b914_1440w.jpg)

该算法的流程如下：

![](https://pic4.zhimg.com/80/v2-bf830ddf519afae2d288a63d0f814b3b_1440w.jpg)

构建![[公式]](https://www.zhihu.com/equation?tex=%5Cvarepsilon)邻域的过程可以使用 `kd-tree`进行优化，循环过程可以使用 `Numba、Cython、C`进行 **[优化](https://link.zhihu.com/?target=https%3A//blog.csdn.net/weixin_38169413/article/details/102729497)** ，`DBSCAN`的 **[源代码](https://link.zhihu.com/?target=https%3A//github.com/HuStanding/nlp-exercise/blob/master/cluster/dbscan/dbscan.py)** ，使用该节一开始提到的数据集，聚类效果如下

![](https://pic4.zhimg.com/80/v2-6e982ce94516b6d3abd8c42483c9a6e3_1440w.jpg)

聚类的过程示意图

![](https://pic3.zhimg.com/v2-97265caf8e739000eadc8065dea6851a_b.jpg)

当设置不同的![[公式]](https://www.zhihu.com/equation?tex=%5Cvarepsilon+)时，会产生不同的结果，如下图所示

![](https://pic2.zhimg.com/80/v2-a38cd64b7d8c21e720f867a74218c145_1440w.jpg)

当设置不同的![[公式]](https://www.zhihu.com/equation?tex=M)时，会产生不同的结果，如下图所示

![](https://pic1.zhimg.com/80/v2-cd257a8c36168252fa3d83a9fe20a534_1440w.jpg)

一般来说，`DBSCAN`算法有以下几个特点：

1. 需要提前确定![[公式]](https://www.zhihu.com/equation?tex=%5Cvarepsilon+)和![[公式]](https://www.zhihu.com/equation?tex=M)值
2. 不需要提前设置聚类的个数
3. 对初值选取敏感，对噪声不敏感
4. 对密度不均的数据聚合效果不好

### **2.2.2 OPTICS算法**

在 `DBSCAN`算法中，使用了统一的![[公式]](https://www.zhihu.com/equation?tex=%5Cvarepsilon)值，当数据密度不均匀的时候，如果设置了较小的![[公式]](https://www.zhihu.com/equation?tex=%5Cvarepsilon)值，则较稀疏的 `cluster`中的节点密度会小于![[公式]](https://www.zhihu.com/equation?tex=M)，会被认为是边界点而不被用于进一步的扩展；如果设置了较大的![[公式]](https://www.zhihu.com/equation?tex=%5Cvarepsilon)值，则密度较大且离的比较近的 `cluster`容易被划分为同一个 `cluster`，如下图所示。

![](https://pic1.zhimg.com/80/v2-a86c5bee2e732de50dd5e8ba50f7ae68_1440w.jpg)

* 如果设置的![[公式]](https://www.zhihu.com/equation?tex=%5Cvarepsilon)较大，将会获得A,B,C这3个 `cluster`
* 如果设置的![[公式]](https://www.zhihu.com/equation?tex=%5Cvarepsilon)较小，将会只获得C1、C2、C3这3个 `cluster`

对于密度不均的数据选取一个合适的![[公式]](https://www.zhihu.com/equation?tex=%5Cvarepsilon)是很困难的，对于高维数据，由于 **维度灾难(Curse of dimensionality)** ,![[公式]](https://www.zhihu.com/equation?tex=%5Cvarepsilon)的选取将变得更加困难。

怎样解决 `DBSCAN`遗留下的问题呢？

> The basic idea to overcome these problems is to run an algorithm which produces a special order of the database with respect to its density-based clustering structure containing the information about every clustering level of the data set (up to a "generating distance" ![[公式]](https://www.zhihu.com/equation?tex=%5Cvarepsilon)), and is very easy to analyze.

即能够提出一种算法，使得基于密度的聚类结构能够呈现出一种特殊的顺序，该顺序所对应的聚类结构包含了每个层级的聚类的信息，并且便于分析。

`OPTICS(Ordering Points To Identify the Clustering Structure, OPTICS)`实际上是 `DBSCAN`算法的一种有效扩展，主要解决对输入参数敏感的问题。即选取有限个邻域参数![[公式]](https://www.zhihu.com/equation?tex=%5Cvarepsilon+_i%28+0+%5Cleq%5Cvarepsilon_%7Bi%7D+%5Cleq+%5Cvarepsilon%29) 进行聚类，这样就能得到不同邻域参数下的聚类结果。

在介绍 `OPTICS`算法之前，再扩展几个概念。

* **核心距离(core-distance)**

样本![[公式]](https://www.zhihu.com/equation?tex=x%E2%88%88X)，对于给定的![[公式]](https://www.zhihu.com/equation?tex=%5Cvarepsilon)和![[公式]](https://www.zhihu.com/equation?tex=M)，使得![[公式]](https://www.zhihu.com/equation?tex=x)成为核心点的最小邻域半径称为![[公式]](https://www.zhihu.com/equation?tex=x)的核心距离，其数学表达如下

![[公式]](https://www.zhihu.com/equation?tex=cd%28x%29%3D%5Cleft%5C%7B%5Cbegin%7Bmatrix%7D+UNDEFINED%2C+%5Cleft+%7C+N_%7B%5Cvarepsilon+%7D%28x%29%5Cright+%7C%3C+M%5C%5C++d%28x%2CN_%7B%5Cvarepsilon+%7D%5E%7BM%7D%28x%29%29%2C+%5Cleft+%7C+N_%7B%5Cvarepsilon+%7D%28x%29%5Cright+%7C+%5Cgeqslant++M+%5Cend%7Bmatrix%7D%5Cright.+%5C%5C)

其中，![[公式]](https://www.zhihu.com/equation?tex=N_%7B%5Cvarepsilon+%7D%5E%7Bi%7D%28x%29)表示在集合![[公式]](https://www.zhihu.com/equation?tex=N_%7B%5Cvarepsilon+%7D%28x%29)中与节点![[公式]](https://www.zhihu.com/equation?tex=x)第![[公式]](https://www.zhihu.com/equation?tex=i)近邻的节点，如![[公式]](https://www.zhihu.com/equation?tex=N_%7B%5Cvarepsilon+%7D%5E%7B1%7D%28x%29)表示![[公式]](https://www.zhihu.com/equation?tex=N_%7B%5Cvarepsilon+%7D%28x%29)中与![[公式]](https://www.zhihu.com/equation?tex=x)最近的节点，如果![[公式]](https://www.zhihu.com/equation?tex=x)为核心点，则必然会有![[公式]](https://www.zhihu.com/equation?tex=cd%28x%29+%5Cleq%5Cvarepsilon)。

* **可达距离(reachability-distance)**

设![[公式]](https://www.zhihu.com/equation?tex=x%2Cy%E2%88%88X)，对于给定的参数![[公式]](https://www.zhihu.com/equation?tex=%5Cvarepsilon)和![[公式]](https://www.zhihu.com/equation?tex=M)，![[公式]](https://www.zhihu.com/equation?tex=y)关于![[公式]](https://www.zhihu.com/equation?tex=x)的可达距离定义为

![[公式]](https://www.zhihu.com/equation?tex=rd%28y%2Cx%29%3D%5Cleft%5C%7B%5Cbegin%7Bmatrix%7D+UNDEFINED%2C+%5Cleft+%7C+N_%7B%5Cvarepsilon+%7D%28x%29%5Cright+%7C%3C+M%5C%5C++%5Cmax%7B%5C%7Bcd%28x%29%2Cd%28x%2Cy%29%5C%7D%7D%2C+%5Cleft%7C++N_%7B%5Cvarepsilon+%7D%28x%29%5Cright+%7C+%5Cgeqslant++M+%5Cend%7Bmatrix%7D%5Cright.+%5C%5C)

特别地，当![[公式]](https://www.zhihu.com/equation?tex=x)为核心点时，可以按照下式来理解![[公式]](https://www.zhihu.com/equation?tex=rd%28x%2Cy%29)的含义

![[公式]](https://www.zhihu.com/equation?tex=rd%28x%2Cy%29%3D%5Cmin%5C%7B%5Ceta%3Ay+%5Cin+N_%7B%5Ceta%7D%28x%29+%E4%B8%94+%5Cleft%7CN_%7B%5Ceta%7D%28x%29%5Cright%7C+%5Cgeq+M%5C%7D+%5C%5C)

即![[公式]](https://www.zhihu.com/equation?tex=rd%28x%2Cy%29)表示使得"![[公式]](https://www.zhihu.com/equation?tex=x)为核心点"且"![[公式]](https://www.zhihu.com/equation?tex=y)从![[公式]](https://www.zhihu.com/equation?tex=x)直接密度可达"同时成立的最小邻域半径。

> 可达距离的意义在于衡量![[公式]](https://www.zhihu.com/equation?tex=y)所在的密度，密度越大，他从相邻节点直接密度可达的距离越小，如果聚类时想要朝着数据尽量稠密的空间进行扩张，那么可达距离最小是最佳的选择。

举个 ，下图中假设![[公式]](https://www.zhihu.com/equation?tex=M%3D3)，半径是![[公式]](https://www.zhihu.com/equation?tex=%CE%B5)。那么![[公式]](https://www.zhihu.com/equation?tex=P)点的核心距离是![[公式]](https://www.zhihu.com/equation?tex=d%281%2CP%29)，点2的可达距离是![[公式]](https://www.zhihu.com/equation?tex=d%281%2CP%29)，点3的可达距离也是![[公式]](https://www.zhihu.com/equation?tex=d%281%2CP%29)，点4的可达距离则是![[公式]](https://www.zhihu.com/equation?tex=d%284%2CP%29)的距离。

![](https://pic1.zhimg.com/80/v2-a99c00a75727cdbed6fde4a95a862538_1440w.jpg)

`OPTICS` **[源代码](https://link.zhihu.com/?target=https%3A//github.com/HuStanding/nlp-exercise/blob/master/cluster/dbscan/optics.py)** ，算法流程如下：

![](https://pic1.zhimg.com/80/v2-1e8fc7235869bbd28bb8a966d3587400_1440w.jpg)

算法中有一个很重要的insert_list()函数，这个函数如下：

![](https://pic1.zhimg.com/80/v2-9d2a7eb53ca16fb78eb7b18eb71ccd74_1440w.jpg)

`OPTICS`算法输出序列的过程：

![](https://pic1.zhimg.com/v2-5a343fcf9224f75969149116b6bf2640_b.jpg)

该算法最终获取知识是一个 **输出序列** ，该序列按照密度不同将相近密度的点聚合在一起，而不是输出该点所属的具体类别，如果要获取该点所属的类型，需要再设置一个参数![[公式]](https://www.zhihu.com/equation?tex=%5Cvarepsilon%27%28%5Cvarepsilon%27+%5Cleq+%5Cvarepsilon%29)提取出具体的类别。这里我们举一个例子就知道是怎么回事了。

随机生成三组密度不均的数据，我们使用 `DBSCAN`和 `OPTICS`来看一下效果。

![](https://pic4.zhimg.com/80/v2-e90887c2ae5e1eef15bfc2618d3983c3_1440w.jpg)

![](https://pic2.zhimg.com/80/v2-69630aca25c9de85ef8a2d1dd34cfcd1_1440w.jpg)

可见，`OPTICS`第一步生成的输出序列较好的保留了各个不同密度的簇的特征，根据输出序列的可达距离图，再设定一个合理的![[公式]](https://www.zhihu.com/equation?tex=%5Cvarepsilon%27)，便可以获得较好的聚类效果。

## **2.3 层次化聚类方法**

前面介绍的几种算法确实可以在较小的复杂度内获取较好的结果，但是这几种算法却存在一个 `链式效应`的现象，比如：A与B相似，B与C相似，那么在聚类的时候便会将A、B、C聚合到一起，但是如果A与C不相似，就会造成聚类误差，严重的时候这个误差可以一直传递下去。为了降低 `链式效应`，这时候层次聚类就该发挥作用了。

![](https://pic3.zhimg.com/80/v2-0141f21130b60f7ac23fa05d7e417cee_1440w.jpg)

**层次聚类算法 (hierarchical clustering)** 将数据集划分为一层一层的 `clusters`，后面一层生成的 `clusters` 基于前面一层的结果。层次聚类算法一般分为两类：

* **Agglomerative 层次聚类** ：又称自底向上（bottom-up）的层次聚类，每一个对象最开始都是一个 `cluster`，每次按一定的准则将最相近的两个 `cluster` 合并生成一个新的 `cluster`，如此往复，直至最终所有的对象都属于一个 `cluster`。这里主要关注此类算法。
* **Divisive 层次聚类** ： 又称自顶向下（top-down）的层次聚类，最开始所有的对象均属于一个 `cluster`，每次按一定的准则将某个 `cluster` 划分为多个 `cluster`，如此往复，直至每个对象均是一个 `cluster`。

![](https://pic2.zhimg.com/80/v2-be2a7cf798fdc983e6521a68b1eb952d_1440w.jpg)

另外，需指出的是，层次聚类算法是一种贪心算法（greedy algorithm），因其每一次合并或划分都是基于某种局部最优的选择。

### **2.3.1 Agglomerative算法**

给定数据集 ![[公式]](https://www.zhihu.com/equation?tex=X%3D%5Cleft+%5C%7Bx%5E%7B%281%29%7D%2Cx%5E%7B%282%29%7D%2C...%2Cx%5E%7B%28n%29%7D%5Cright+%5C%7D)，`Agglomerative`层次聚类最简单的实现方法分为以下几步：

![](https://pic3.zhimg.com/80/v2-da7329a152f5b9a4d7f4ccc7cbac117a_1440w.jpg)

`Agglomerative`算法 **[源代码](https://link.zhihu.com/?target=https%3A//github.com/HuStanding/nlp-exercise/blob/master/cluster/hierarchical/hierarchical.py)** ，可以看到，该 算法的时间复杂度为 ![[公式]](https://www.zhihu.com/equation?tex=O%28n%5E3%29) （由于每次合并两个 `cluster` 时都要遍历大小为 ![[公式]](https://www.zhihu.com/equation?tex=O%28n%5E2%29+)的距离矩阵来搜索最小距离，而这样的操作需要进行 ![[公式]](https://www.zhihu.com/equation?tex=n%E2%88%921) 次），空间复杂度为![[公式]](https://www.zhihu.com/equation?tex=O%28n%5E2%29+) （由于要存储距离矩阵）。

![](https://pic3.zhimg.com/80/v2-26d260f8cad14d0af66928e34fa8b68e_1440w.jpg)

上图中分别使用了层次聚类中4个不同的 `cluster`度量方法，可以看到，使用 `single-link`确实会造成一定的链式效应，而使用 `complete-link`则完全不会产生这种现象，使用 `average-link`和 `ward-link`则介于两者之间。

## **2.4 聚类方法比较**

![](https://pic2.zhimg.com/80/v2-04a2252ba991e07045613463e0414dc9_1440w.jpg)

## **三、参考文献**

[1] 李航.统计学习方法

[2] Peter Harrington.Machine Learning in Action/李锐.机器学习实战

[3] [https://www.**zhihu.com/question/3455**4321](https://www.zhihu.com/question/34554321)

[4] **[T. Soni Madhulatha.AN OVERVIEW ON CLUSTERING METHODS](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1205.1117.pdf)**

[5] [https://**zhuanlan.zhihu.com/p/32**375430](https://zhuanlan.zhihu.com/p/32375430)

[6] **[http://heathcliff.me/聚类分析（一）：层次聚类算法](https://link.zhihu.com/?target=http%3A//heathcliff.me/%25E8%2581%259A%25E7%25B1%25BB%25E5%2588%2586%25E6%259E%2590%25EF%25BC%2588%25E4%25B8%2580%25EF%25BC%2589%25EF%25BC%259A%25E5%25B1%2582%25E6%25AC%25A1%25E8%2581%259A%25E7%25B1%25BB%25E7%25AE%2597%25E6%25B3%2595/)**

[7] [https://www.**cnblogs.com/tiaozistudy**/p/dbscan_algorithm.html](https://link.zhihu.com/?target=https%3A//www.cnblogs.com/tiaozistudy/p/dbscan_algorithm.html)

[8] [https://**blog.csdn.net/itplus/ar**ticle/details/10089323](https://link.zhihu.com/?target=https%3A//blog.csdn.net/itplus/article/details/10089323)

[9] **[Mihael Ankerst.OPTICS: ordering points to identify the clustering structure](https://link.zhihu.com/?target=http%3A//www2.denizyuret.com/ref/ankerst/OPTICS.pdf)**
