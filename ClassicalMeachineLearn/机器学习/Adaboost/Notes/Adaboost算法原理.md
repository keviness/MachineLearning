# AdaBoost算法解析

**一、AdaBoost简介**

**Boosting, 也称为增强学习或提升法，是一种重要的集成学习技术， 能够将预测精度仅比随机猜度略高的弱学习器增强为预测精度高的强学习器，这在直接构造强学习器非常困难的情况下，为学习算法的设计提供了一种有效的新思路和新方法。其中最为成功应用的是，Yoav Freund和Robert Schapire在1995年提出的AdaBoost算法。
AdaBoost是英文"Adaptive Boosting"（自适应增强）的缩写，它的自适应在于：前一个基本分类器被错误分类的样本的权值会增大，而正确分类的样本的权值会减小，并再次用来训练下一个基本分类器。同时，在每一轮迭代中，加入一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数才确定最终的强分类器。
**Adaboost算法可以简述为三个步骤：**
（1）首先，是初始化训练数据的权值分布D1。假设有N个训练样本数据，则每一个训练样本最开始时，都被赋予相同的权值：w1=1/N。
（2）然后，训练弱分类器hi。具体训练过程中是：如果某个训练样本点，被弱分类器hi准确地分类，那么在构造下一个训练集中，它对应的权值要减小；相反，如果某个训练样本点被错误分类，那么它的权值就应该增大。权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。
（3）最后，将各个训练得到的弱分类器组合成一个强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。
换而言之，误差率低的弱分类器在最终分类器中占的权重较大，否则较小。**

二、**AdaBoost算法过程**

给定训练数据集：（x1,y1）,(x2,y2)···(xn,yn)，其中yi属于{1，-1}用于表示训练样本的类别标签， *i=1,...,N* 。Adaboost的目的就是从训练数据中学习一系列弱分类器或基本分类器，然后将这些弱分类器组合成一个强分类器。

**相关符号定义：**

![](https://pic1.zhimg.com/80/v2-9023240e38cdaeba9b4dc54c5b4d4fbc_1440w.jpg)

**Adaboost的算法流程如下：**

![](https://pic3.zhimg.com/80/v2-8105b7ab4069e61cdf9e7e3d53fdb85a_1440w.jpg)

![](https://pic3.zhimg.com/80/v2-ec52646cb7fd8d1645955562e662f7a6_1440w.jpg)

![](https://pic1.zhimg.com/80/v2-6cc626bbe52e2c9d66d5d1cc927146e8_1440w.jpg)

**相关说明：**

![](https://pic3.zhimg.com/80/v2-7c98a8457a102fe7a63f93862fe052fe_1440w.jpg)

综合上面的推导，可得样本分错与分对时，其权值更新的公式为：

![](https://pic2.zhimg.com/80/v2-b73207c73c4d2c88c36f88a5177610d5_1440w.jpg)

三、**AdaBoost实例讲解**

**例：给定如图所示的训练样本，弱分类器采用平行于坐标轴的直线，用Adaboost算法的实现强分类过程。**

![](https://pic2.zhimg.com/80/v2-0ce6b40060553d0c7fbe5ab17bf10f01_1440w.jpg)

**数据分析：**

将这10个样本作为训练数据，根据 *X* 和*Y* 的对应关系，可把这10个数据分为两类，图中用“+”表示类别1，用“O”表示类别-1。本例使用水平或者垂直的直线作为分类器，图中已经给出了三个弱分类器，即：

![](https://pic4.zhimg.com/80/v2-0b6f130bbb16bb7ed507af0144849777_1440w.jpg)

**初始化：**

首先需要初始化训练样本数据的权值分布，每一个训练样本最开始时都被赋予相同的权值： *wi* =1/ *N* ，这样训练样本集的初始权值分布*D*1( *i* )：

令每个权值*w*1*i* = 1/*N* = 0.1，其中，*N* = 10，*i* = 1,2, ..., 10，然后分别对于 *t* = 1,2,3, ...等值进行迭代（*t*表示迭代次数，表示第*t*轮），下表已经给出训练样本的权值分布情况：

![](https://pic2.zhimg.com/80/v2-18e2cacf71e40b774b58e1ca2c68f175_1440w.jpg)

**第1次迭代 *t* =1：**

初试的权值分布*D1*为1/N（10个数据，每个数据的权值皆初始化为0.1），

D1=[0.1, 0.1, 0.1, 0.1, 0.1, 0.1,0.1, 0.1, 0.1, 0.1]

在权值分布*D1*的情况下，取已知的三个弱分类器*h*1、*h*2和*h*3中误差率最小的分类器作为第1个基本分类器*H*1( *x* )（三个弱分类器的误差率都是0.3，那就取第1个吧）

![](https://pic4.zhimg.com/80/v2-8347b1e81a2725807b005f99a6d65d83_1440w.jpg)

在分类器*H*1( *x* )=*h*1情况下，样本点“5 7 8”被错分，因此基本分类器*H*1( *x* )的误差率为：

![](https://pic2.zhimg.com/80/v2-9db68140d4a69e930c53cbd886afdb05_1440w.jpg)

可见，被误分类样本的权值之和影响误差率 *e* ，误差率*e*影响基本分类器在最终分类器中所占的权重 *α* 。

![](https://pic4.zhimg.com/80/v2-f91b3adbf817d8676a7209a8a85fd273_1440w.jpg)

然后，更新训练样本数据的权值分布，用于下一轮迭代，对于正确分类的训练样本“1 2 3 4 6 9 10”（共7个）的权值更新为：

![](https://pic2.zhimg.com/80/v2-09932feb7af51772bd07a36e7901eb15_1440w.jpg)

这样，第1轮迭代后，最后得到各个样本数据新的权值分布：

D2=[1/14,1/14,1/14,1/14,1/6,1/14,1/6,1/6,1/14,1/14]

由于样本数据“5 7 8”被*H*1(x)分错了，所以它们的权值由之前的0.1增大到1/6；反之，其它数据皆被分正确，所以它们的权值皆由之前的0.1减小到1/14，下表给出了权值分布的变换情况：

![](https://pic3.zhimg.com/80/v2-fd8eca9d4d957f86ea916a0ee874aaa6_1440w.jpg)

可得 **分类函数：** *f*1( *x* )= *α*1*H*1( *x* ) = 0.4236*H*1( *x* )。此时，组合一个基本分类器 *sign* ( *f1* ( *x* ))作为强分类器在训练数据集上有3个误分类点（即5 7 8），此时强分类器的训练错误为：0.3

**第二次迭代 *t* =2：**

在权值分布D2的情况下，再取三个弱分类器h1、h2和h3中误差率最小的分类器作为第2个基本分类器H2(x)：

①　当取弱分类器h1=X1=2.5时，此时被错分的样本点为“5 7 8”：

误差率e=1/6+1/6+1/6=3/6=1/2；

②　当取弱分类器h2=X1=8.5时，此时被错分的样本点为“3 4 6”：

误差率e=1/14+1/14+1/14=3/14；

③　当取弱分类器h3=X2=6.5时，此时被错分的样本点为“1 2 9”：

误差率e=1/14+1/14+1/14=3/14；

![](https://pic1.zhimg.com/80/v2-5d3db5cd1a2334b0198d6f2158b6640c_1440w.jpg)

因此，取当前最小的分类器h2作为第2个基本分类器H2(x)

![](https://pic1.zhimg.com/80/v2-83244b6bb931c40d8b36bcd4a2b1a500_1440w.jpg)

显然，*H*2( *x* )把样本“3 4 6”分错了，根据*D*2可知它们的权值为*D*2(3)=1/14，*D*2(4)=1/14， *D*2(6)=1/14，所以*H*2( *x* )在训练数据集上的误差率：

![](https://pic4.zhimg.com/80/v2-25fd6f5157cd419abfbfdec583a9001b_1440w.jpg)

这样，第2轮迭代后，最后得到各个样本数据新的权值分布：

D3=[1/22,1/22,1/6,1/6,7/66,1/6,7/66,7/66,1/22,1/22]

下表给出了权值分布的变换情况：

![](https://pic3.zhimg.com/80/v2-31781c9311febb9429ef00e0aaec71ee_1440w.jpg)

可得 **分类函数：** *f*2( *x* )=0.4236*H*1(x) + 0.6496*H*2(x)。此时，组合两个基本分类器 *sign* (*f*2(x))作为强分类器在训练数据集上有3个误分类点（即3 4 6），此时强分类器的训练错误为：0.3

**第三次迭代 *t* =3:**

在权值分布D3的情况下，再取三个弱分类器h1、h2和h3中误差率最小的分类器作为第3个基本分类器H3(x)：

①　当取弱分类器h1=X1=2.5时，此时被错分的样本点为“5 7 8”：

误差率e=7/66+7/66+7/66=7/22；

②　当取弱分类器h2=X1=8.5时，此时被错分的样本点为“3 4 6”：

误差率e=1/6+1/6+1/6=1/2=0.5；

③　当取弱分类器h3=X2=6.5时，此时被错分的样本点为“1 2 9”：

误差率e=1/22+1/22+1/22=3/22；

![](https://pic3.zhimg.com/80/v2-23c4c2737135cc5d0fc7eca7aa8e6c32_1440w.jpg)

因此，取当前最小的分类器*h3*作为第3个基本分类器*H*3( *x* )：

![](https://pic4.zhimg.com/80/v2-66a109d996c73ef45ab48312b8182697_1440w.jpg)

这样，第3轮迭代后，得到各个样本数据新的权值分布为：

D4=[1/6,1/6,11/114,11/114,7/114,11/114,7/114,7/114,1/6,1/38]

下表给出了权值分布的变换情况：

![](https://pic2.zhimg.com/80/v2-514e8ec47748340cfbcf11d610cbe3b1_1440w.jpg)

可得 **分类函数：** *f*3( *x* )=0.4236*H*1( *x* ) + 0.6496*H*2( *x* )+0.9229*H*3( *x* )。此时，组合三个基本分类器 *sign* (*f*3(x))作为强分类器，在训练数据集上有0个误分类点。至此，整个训练过程结束。

整合所有分类器，可得最终的强分类器为：

![](https://pic2.zhimg.com/80/v2-3dde2a08a1b6172ab52951080d0b0495_1440w.jpg)

**这个强分类器*Hfinal*对训练样本的错误率为0！**

**三、AdaBoost的优点和缺点**

**优点**

（1）Adaboost提供一种框架，在框架内可以使用各种方法构建子分类器。可以使用简单的弱分类器，不用对特征进行筛选，也 **不存在过拟合的现象** 。

（2）Adaboost算法不需要弱分类器的先验知识，最后得到的强分类器的分类精度依赖于所有弱分类器。无论是应用于人造数据还是真实数据，Adaboost都能显著的提高学习精度。

（3）Adaboost算法不需要预先知道弱分类器的错误率上限，且最后得到的强分类器的分类精度依赖于所有弱分类器的分类精度，可以深挖分类器的能力。Adaboost可以根据弱分类器的反馈，自适应地调整假定的错误率，执行的效率高。

（4）Adaboost对同一个训练样本集训练不同的弱分类器，按照一定的方法把这些弱分类器集合起来，构造一个分类能力很强的强分类器，即“三个臭皮匠赛过一个诸葛亮”。

**缺点：**

在Adaboost训练过程中，Adaboost会使得难于分类样本的权值呈指数增长，训练将会过于偏向这类困难的样本，导致Adaboost算法易受噪声干扰。此外，Adaboost依赖于弱分类器，而弱分类器的训练时间往往很长。

Adaboost算法的某些特性是非常好的，这里主要介绍Adaboost的两个特性。（1）是训练的错误率上界，随着迭代次数的增加，会逐渐下降；（2）是Adaboost算法即使训练次数很多，也不会出现过拟合的问题。关于这两方面的研究和分析，我建议各大网友，还是看看大神的博客：[http://**blog.csdn.net/v_july_v/**article/details/40718799](https://link.zhihu.com/?target=http%3A//blog.csdn.net/v_july_v/article/details/40718799)

原文博主文章：[Adaboost算法原理分析和实例+代码（简明易懂） - CSDN博客](https://link.zhihu.com/?target=https%3A//blog.csdn.net/guyuealian/article/details/70995333)
