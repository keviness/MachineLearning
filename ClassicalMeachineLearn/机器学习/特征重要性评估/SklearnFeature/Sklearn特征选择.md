# ç‰¹å¾é€‰æ‹© (feature_selection)

ç›®å½•

* [ç‰¹å¾é€‰æ‹© (feature_selection)](https://www.cnblogs.com/stevenlk/p/6543628.html#%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9-feature_selection)
  * [Filter](https://www.cnblogs.com/stevenlk/p/6543628.html#filter)
    * [1. ç§»é™¤ä½æ–¹å·®çš„ç‰¹å¾ (Removing features with low variance)](https://www.cnblogs.com/stevenlk/p/6543628.html#1-%E7%A7%BB%E9%99%A4%E4%BD%8E%E6%96%B9%E5%B7%AE%E7%9A%84%E7%89%B9%E5%BE%81-removing-features-with-low-variance)
    * [2. å•å˜é‡ç‰¹å¾é€‰æ‹© (Univariate feature selection)](https://www.cnblogs.com/stevenlk/p/6543628.html#2-%E5%8D%95%E5%8F%98%E9%87%8F%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9-univariate-feature-selection)
      * [2.1 å¡æ–¹(Chi2)æ£€éªŒ](https://www.cnblogs.com/stevenlk/p/6543628.html#21-%E5%8D%A1%E6%96%B9chi2%E6%A3%80%E9%AA%8C)
      * [2.2 Pearsonç›¸å…³ç³»æ•° (Pearson Correlation)](https://www.cnblogs.com/stevenlk/p/6543628.html#22-pearson%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0-pearson-correlation)
      * [2.3 äº’ä¿¡æ¯å’Œæœ€å¤§ä¿¡æ¯ç³»æ•° (Mutual information and maximal information coefficient (MIC)](https://www.cnblogs.com/stevenlk/p/6543628.html#23-%E4%BA%92%E4%BF%A1%E6%81%AF%E5%92%8C%E6%9C%80%E5%A4%A7%E4%BF%A1%E6%81%AF%E7%B3%BB%E6%95%B0-mutual-information-and-maximal-information-coefficient-mic)
      * [2.4 è·ç¦»ç›¸å…³ç³»æ•° (Distance Correlation)](https://www.cnblogs.com/stevenlk/p/6543628.html#24-%E8%B7%9D%E7%A6%BB%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0-distance-correlation)
      * [2.5 åŸºäºæ¨¡å‹çš„ç‰¹å¾æ’åº (Model based ranking)](https://www.cnblogs.com/stevenlk/p/6543628.html#25-%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%89%B9%E5%BE%81%E6%8E%92%E5%BA%8F-model-based-ranking)
  * [Wrapper](https://www.cnblogs.com/stevenlk/p/6543628.html#wrapper)
    * [3. é€’å½’ç‰¹å¾æ¶ˆé™¤ (Recursive Feature Elimination)](https://www.cnblogs.com/stevenlk/p/6543628.html#3-%E9%80%92%E5%BD%92%E7%89%B9%E5%BE%81%E6%B6%88%E9%99%A4-recursive-feature-elimination)
  * [Embedded](https://www.cnblogs.com/stevenlk/p/6543628.html#embedded)
    * [4. ä½¿ç”¨SelectFromModelé€‰æ‹©ç‰¹å¾ (Feature selection using SelectFromModel)](https://www.cnblogs.com/stevenlk/p/6543628.html#4-%E4%BD%BF%E7%94%A8selectfrommodel%E9%80%89%E6%8B%A9%E7%89%B9%E5%BE%81-feature-selection-using-selectfrommodel)
      * [4.1 åŸºäºL1çš„ç‰¹å¾é€‰æ‹© (L1-based feature selection)](https://www.cnblogs.com/stevenlk/p/6543628.html#41-%E5%9F%BA%E4%BA%8El1%E7%9A%84%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9-l1-based-feature-selection)
      * [4.2 éšæœºç¨€ç–æ¨¡å‹ (Randomized sparse models)](https://www.cnblogs.com/stevenlk/p/6543628.html#42-%E9%9A%8F%E6%9C%BA%E7%A8%80%E7%96%8F%E6%A8%A1%E5%9E%8B-randomized-sparse-models)
      * [4.3 åŸºäºæ ‘çš„ç‰¹å¾é€‰æ‹© (Tree-based feature selection)](https://www.cnblogs.com/stevenlk/p/6543628.html#43-%E5%9F%BA%E4%BA%8E%E6%A0%91%E7%9A%84%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9-tree-based-feature-selection)
    * [5. å°†ç‰¹å¾é€‰æ‹©è¿‡ç¨‹èå…¥pipeline (Feature selection as part of a pipeline)](https://www.cnblogs.com/stevenlk/p/6543628.html#5-%E5%B0%86%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E8%BF%87%E7%A8%8B%E8%9E%8D%E5%85%A5pipeline-feature-selection-as-part-of-a-pipeline)

> æœ¬æ–‡ä¸»è¦å‚è€ƒsklearn(0.18ç‰ˆä¸ºä¸»ï¼Œéƒ¨åˆ†0.17)çš„1.13èŠ‚çš„å®˜æ–¹æ–‡æ¡£ï¼Œä»¥åŠä¸€äº›å·¥ç¨‹å®è·µæ•´ç†è€Œæˆã€‚

ã€€ã€€å½“æ•°æ®é¢„å¤„ç†å®Œæˆåï¼Œæˆ‘ä»¬éœ€è¦é€‰æ‹©æœ‰æ„ä¹‰çš„ç‰¹å¾è¾“å…¥æœºå™¨å­¦ä¹ çš„ç®—æ³•å’Œæ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚é€šå¸¸æ¥è¯´ï¼Œä»ä¸¤ä¸ªæ–¹é¢è€ƒè™‘æ¥é€‰æ‹©ç‰¹å¾ï¼š

* **ç‰¹å¾æ˜¯å¦å‘æ•£** ï¼šå¦‚æœä¸€ä¸ªç‰¹å¾ä¸å‘æ•£ï¼Œä¾‹å¦‚æ–¹å·®æ¥è¿‘äº0ï¼Œä¹Ÿå°±æ˜¯è¯´æ ·æœ¬åœ¨è¿™ä¸ªç‰¹å¾ä¸ŠåŸºæœ¬ä¸Šæ²¡æœ‰å·®å¼‚ï¼Œè¿™ä¸ªç‰¹å¾å¯¹äºæ ·æœ¬çš„åŒºåˆ†å¹¶æ²¡æœ‰ä»€ä¹ˆç”¨ã€‚
* **ç‰¹å¾ä¸ç›®æ ‡çš„ç›¸å…³æ€§** ï¼šè¿™ç‚¹æ¯”è¾ƒæ˜¾è§ï¼Œä¸ç›®æ ‡ç›¸å…³æ€§é«˜çš„ç‰¹å¾ï¼Œåº”å½“ä¼˜é€‰é€‰æ‹©ã€‚é™¤ç§»é™¤ä½æ–¹å·®æ³•å¤–ï¼Œæœ¬æ–‡ä»‹ç»çš„å…¶ä»–æ–¹æ³•å‡ä»ç›¸å…³æ€§è€ƒè™‘ã€‚

æ ¹æ®ç‰¹å¾é€‰æ‹©çš„å½¢å¼åˆå¯ä»¥å°†ç‰¹å¾é€‰æ‹©æ–¹æ³•åˆ†ä¸º3ç§ï¼š

* **Filter** ï¼šè¿‡æ»¤æ³•ï¼ŒæŒ‰ç…§å‘æ•£æ€§æˆ–è€…ç›¸å…³æ€§å¯¹å„ä¸ªç‰¹å¾è¿›è¡Œè¯„åˆ†ï¼Œè®¾å®šé˜ˆå€¼æˆ–è€…å¾…é€‰æ‹©é˜ˆå€¼çš„ä¸ªæ•°ï¼Œé€‰æ‹©ç‰¹å¾ã€‚
* **Wrapper** ï¼šåŒ…è£…æ³•ï¼Œæ ¹æ®ç›®æ ‡å‡½æ•°ï¼ˆé€šå¸¸æ˜¯é¢„æµ‹æ•ˆæœè¯„åˆ†ï¼‰ï¼Œæ¯æ¬¡é€‰æ‹©è‹¥å¹²ç‰¹å¾ï¼Œæˆ–è€…æ’é™¤è‹¥å¹²ç‰¹å¾ã€‚
* **Embedded** ï¼šåµŒå…¥æ³•ï¼Œå…ˆä½¿ç”¨æŸäº›æœºå™¨å­¦ä¹ çš„ç®—æ³•å’Œæ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œå¾—åˆ°å„ä¸ªç‰¹å¾çš„æƒå€¼ç³»æ•°ï¼Œæ ¹æ®ç³»æ•°ä»å¤§åˆ°å°é€‰æ‹©ç‰¹å¾ã€‚ç±»ä¼¼äºFilteræ–¹æ³•ï¼Œä½†æ˜¯æ˜¯é€šè¿‡è®­ç»ƒæ¥ç¡®å®šç‰¹å¾çš„ä¼˜åŠ£ã€‚

ç‰¹å¾é€‰æ‹©ä¸»è¦æœ‰ä¸¤ä¸ªç›®çš„ï¼š

* å‡å°‘ç‰¹å¾æ•°é‡ã€é™ç»´ï¼Œä½¿æ¨¡å‹æ³›åŒ–èƒ½åŠ›æ›´å¼ºï¼Œå‡å°‘è¿‡æ‹Ÿåˆï¼›
* å¢å¼ºå¯¹ç‰¹å¾å’Œç‰¹å¾å€¼ä¹‹é—´çš„ç†è§£ã€‚

ã€€ã€€æ‹¿åˆ°æ•°æ®é›†ï¼Œä¸€ä¸ªç‰¹å¾é€‰æ‹©æ–¹æ³•ï¼Œå¾€å¾€å¾ˆéš¾åŒæ—¶å®Œæˆè¿™ä¸¤ä¸ªç›®çš„ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œé€‰æ‹©ä¸€ç§è‡ªå·±æœ€ç†Ÿæ‚‰æˆ–è€…æœ€æ–¹ä¾¿çš„ç‰¹å¾é€‰æ‹©æ–¹æ³•ï¼ˆå¾€å¾€ç›®çš„æ˜¯é™ç»´ï¼Œè€Œå¿½ç•¥äº†å¯¹ç‰¹å¾å’Œæ•°æ®ç†è§£çš„ç›®çš„ï¼‰ã€‚æœ¬æ–‡å°†ç»“åˆ Scikit-learnæä¾›çš„ä¾‹å­ ä»‹ç»å‡ ç§å¸¸ç”¨çš„ç‰¹å¾é€‰æ‹©æ–¹æ³•ï¼Œå®ƒä»¬å„è‡ªçš„ä¼˜ç¼ºç‚¹å’Œé—®é¢˜ã€‚

## Filter

### 1. ç§»é™¤ä½æ–¹å·®çš„ç‰¹å¾ (Removing features with low variance)

ã€€ã€€å‡è®¾æŸç‰¹å¾çš„ç‰¹å¾å€¼åªæœ‰0å’Œ1ï¼Œå¹¶ä¸”åœ¨æ‰€æœ‰è¾“å…¥æ ·æœ¬ä¸­ï¼Œ95%çš„å®ä¾‹çš„è¯¥ç‰¹å¾å–å€¼éƒ½æ˜¯1ï¼Œé‚£å°±å¯ä»¥è®¤ä¸ºè¿™ä¸ªç‰¹å¾ä½œç”¨ä¸å¤§ã€‚å¦‚æœ100%éƒ½æ˜¯1ï¼Œé‚£è¿™ä¸ªç‰¹å¾å°±æ²¡æ„ä¹‰äº†ã€‚ **å½“ç‰¹å¾å€¼éƒ½æ˜¯ç¦»æ•£å‹å˜é‡çš„æ—¶å€™è¿™ç§æ–¹æ³•æ‰èƒ½ç”¨ï¼Œå¦‚æœæ˜¯è¿ç»­å‹å˜é‡ï¼Œå°±éœ€è¦å°†è¿ç»­å˜é‡ç¦»æ•£åŒ–ä¹‹åæ‰èƒ½ç”¨** ã€‚è€Œä¸”å®é™…å½“ä¸­ï¼Œä¸€èˆ¬ä¸å¤ªä¼šæœ‰95%ä»¥ä¸Šéƒ½å–æŸä¸ªå€¼çš„ç‰¹å¾å­˜åœ¨ï¼Œæ‰€ä»¥è¿™ç§æ–¹æ³•è™½ç„¶ç®€å•ä½†æ˜¯ä¸å¤ªå¥½ç”¨ã€‚å¯ä»¥æŠŠå®ƒä½œä¸ºç‰¹å¾é€‰æ‹©çš„é¢„å¤„ç†ï¼Œå…ˆå»æ‰é‚£äº›å–å€¼å˜åŒ–å°çš„ç‰¹å¾ï¼Œç„¶åå†ä»æ¥ä¸‹æ¥æåˆ°çš„çš„ç‰¹å¾é€‰æ‹©æ–¹æ³•ä¸­é€‰æ‹©åˆé€‚çš„è¿›è¡Œè¿›ä¸€æ­¥çš„ç‰¹å¾é€‰æ‹©ã€‚

```python
>>> from sklearn.feature_selection import VarianceThreshold
>>> X = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]
>>> sel = VarianceThreshold(threshold=(.8 * (1 - .8)))
>>> sel.fit_transform(X)
array([[0, 1],
       [1, 0],
       [0, 0],
       [1, 1],
       [1, 0],
       [1, 1]])
```

æœç„¶, VarianceThreshold ç§»é™¤äº†ç¬¬ä¸€åˆ—ç‰¹å¾ï¼Œç¬¬ä¸€åˆ—ä¸­ç‰¹å¾å€¼ä¸º0çš„æ¦‚ç‡è¾¾åˆ°äº†5/6.

### 2. å•å˜é‡ç‰¹å¾é€‰æ‹© (Univariate feature selection)

ã€€ã€€**å•å˜é‡ç‰¹å¾é€‰æ‹©çš„åŸç†æ˜¯åˆ†åˆ«å•ç‹¬çš„è®¡ç®—æ¯ä¸ªå˜é‡çš„æŸä¸ªç»Ÿè®¡æŒ‡æ ‡ï¼Œæ ¹æ®è¯¥æŒ‡æ ‡æ¥åˆ¤æ–­å“ªäº›æŒ‡æ ‡é‡è¦ï¼Œå‰”é™¤é‚£äº›ä¸é‡è¦çš„æŒ‡æ ‡ã€‚**

ã€€ã€€å¯¹äº **åˆ†ç±»é—®é¢˜(yç¦»æ•£)** ï¼Œå¯é‡‡ç”¨ï¼š
ã€€ã€€ã€€ã€€_å¡æ–¹æ£€éªŒ_ï¼Œ *f_classif* ,  *mutual_info_classif* ï¼Œ*äº’ä¿¡æ¯*
ã€€ã€€å¯¹äº **å›å½’é—®é¢˜(yè¿ç»­)** ï¼Œå¯é‡‡ç”¨ï¼š
ã€€ã€€ã€€ã€€_çš®å°”æ£®ç›¸å…³ç³»æ•°_ï¼Œ *f_regression* ,  *mutual_info_regression* ï¼Œ*æœ€å¤§ä¿¡æ¯ç³»æ•°*

ã€€ã€€ **è¿™ç§æ–¹æ³•æ¯”è¾ƒç®€å•ï¼Œæ˜“äºè¿è¡Œï¼Œæ˜“äºç†è§£ï¼Œé€šå¸¸å¯¹äºç†è§£æ•°æ®æœ‰è¾ƒå¥½çš„æ•ˆæœï¼ˆä½†å¯¹ç‰¹å¾ä¼˜åŒ–ã€æé«˜æ³›åŒ–èƒ½åŠ›æ¥è¯´ä¸ä¸€å®šæœ‰æ•ˆï¼‰** ã€‚è¿™ç§æ–¹æ³•æœ‰è®¸å¤šæ”¹è¿›çš„ç‰ˆæœ¬ã€å˜ç§ã€‚

ã€€ã€€å•å˜é‡ç‰¹å¾é€‰æ‹©åŸºäºå•å˜é‡çš„ç»Ÿè®¡æµ‹è¯•æ¥é€‰æ‹©æœ€ä½³ç‰¹å¾ã€‚å®ƒå¯ä»¥çœ‹ä½œé¢„æµ‹æ¨¡å‹çš„ä¸€é¡¹é¢„å¤„ç†ã€‚ =Scikit-learnå°†ç‰¹å¾é€‰æ‹©ç¨‹åºç”¨åŒ…å« transform å‡½æ•°çš„å¯¹è±¡æ¥å±•ç°= ï¼š

* SelectKBest ç§»é™¤å¾—åˆ†å‰ k åä»¥å¤–çš„æ‰€æœ‰ç‰¹å¾(å–top k)
* SelectPercentile ç§»é™¤å¾—åˆ†åœ¨ç”¨æˆ·æŒ‡å®šç™¾åˆ†æ¯”ä»¥åçš„ç‰¹å¾(å–top k%)
* å¯¹æ¯ä¸ªç‰¹å¾ä½¿ç”¨é€šç”¨çš„å•å˜é‡ç»Ÿè®¡æ£€éªŒï¼š å‡æ­£ç‡(false positive rate) SelectFpr, ä¼ªå‘ç°ç‡(false discovery rate) SelectFdr, æˆ–æ—ç³»è¯¯å·®ç‡ SelectFwe.
* GenericUnivariateSelect å¯ä»¥è®¾ç½®ä¸åŒçš„ç­–ç•¥æ¥è¿›è¡Œå•å˜é‡ç‰¹å¾é€‰æ‹©ã€‚åŒæ—¶ä¸åŒçš„é€‰æ‹©ç­–ç•¥ä¹Ÿèƒ½å¤Ÿä½¿ç”¨è¶…å‚æ•°å¯»ä¼˜ï¼Œä»è€Œè®©æˆ‘ä»¬æ‰¾åˆ°æœ€ä½³çš„å•å˜é‡ç‰¹å¾é€‰æ‹©ç­–ç•¥ã€‚

ã€€ã€€å°†ç‰¹å¾è¾“å…¥åˆ°è¯„åˆ†å‡½æ•°ï¼Œè¿”å›ä¸€ä¸ªå•å˜é‡çš„f_score(Fæ£€éªŒçš„å€¼)æˆ–p-values(På€¼ï¼Œå‡è®¾æ£€éªŒä¸­çš„ä¸€ä¸ªæ ‡å‡†ï¼ŒP-valueç”¨æ¥å’Œæ˜¾è‘—æ€§æ°´å¹³ä½œæ¯”è¾ƒ)ï¼Œæ³¨æ„SelectKBest å’Œ SelectPercentileåªæœ‰å¾—åˆ†ï¼Œæ²¡æœ‰p-valueã€‚

* For classification: chi2, f_classif, mutual_info_classif
* For regression: f_regression, mutual_info_regression

> Notice:
> ã€€ã€€The methods based on F-test estimate the degree of linear dependency between two random variables. (Fæ£€éªŒç”¨äºè¯„ä¼°ä¸¤ä¸ªéšæœºå˜é‡çš„çº¿æ€§ç›¸å…³æ€§)On the other hand, mutual information methods can capture any kind of statistical dependency, but being nonparametric, they require more samples for accurate estimation.(å¦ä¸€æ–¹é¢ï¼Œäº’ä¿¡æ¯çš„æ–¹æ³•å¯ä»¥æ•è·ä»»ä½•ç±»å‹çš„ç»Ÿè®¡ä¾èµ–å…³ç³»ï¼Œä½†æ˜¯ä½œä¸ºä¸€ä¸ªéå‚æ•°æ–¹æ³•ï¼Œä¼°è®¡å‡†ç¡®éœ€è¦æ›´å¤šçš„æ ·æœ¬)

> Feature selection with sparse data:
> ã€€ã€€If you use sparse data (i.e. data represented as sparse matrices), chi2, mutual_info_regression, mutual_info_classif will deal with the data without making it dense.(å¦‚æœä½ ä½¿ç”¨ç¨€ç–æ•°æ®(æ¯”å¦‚ï¼Œä½¿ç”¨ç¨€ç–çŸ©é˜µè¡¨ç¤ºçš„æ•°æ®), å¡æ–¹æ£€éªŒ(chi2)ã€äº’ä¿¡æ¯å›å½’(mutual_info_regression)ã€äº’ä¿¡æ¯åˆ†ç±»(mutual_info_classif)åœ¨å¤„ç†æ•°æ®æ—¶å¯ä¿æŒå…¶ç¨€ç–æ€§.)

> Examples:
> [Univariate Feature Selection](http://scikit-learn.org/stable/auto_examples/feature_selection/plot_feature_selection.html#sphx-glr-auto-examples-feature-selection-plot-feature-selection-py)
> [Comparison of F-test and mutual information](http://scikit-learn.org/stable/auto_examples/feature_selection/plot_f_test_vs_mi.html#sphx-glr-auto-examples-feature-selection-plot-f-test-vs-mi-py)

#### 2.1 å¡æ–¹(Chi2)æ£€éªŒ

ã€€ã€€ç»å…¸çš„å¡æ–¹æ£€éªŒæ˜¯æ£€éªŒå®šæ€§è‡ªå˜é‡å¯¹å®šæ€§å› å˜é‡çš„ç›¸å…³æ€§ã€‚æ¯”å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹æ ·æœ¬è¿›è¡Œä¸€æ¬¡**ğ‘**â„**ğ‘–**2chi2 æµ‹è¯•æ¥é€‰æ‹©æœ€ä½³çš„ä¸¤é¡¹ç‰¹å¾ï¼š

```python
>>> from sklearn.datasets import load_iris
>>> from sklearn.feature_selection import SelectKBest
>>> from sklearn.feature_selection import chi2
>>> iris = load_iris()
>>> X, y = iris.data, iris.target
>>> X.shape
(150, 4)
>>> X_new = SelectKBest(chi2, k=2).fit_transform(X, y)
>>> X_new.shape
(150, 2)
```

#### 2.2 Pearsonç›¸å…³ç³»æ•° (Pearson Correlation)

ã€€ã€€çš®å°”æ£®ç›¸å…³ç³»æ•°æ˜¯ä¸€ç§æœ€ç®€å•çš„ï¼Œèƒ½å¸®åŠ©ç†è§£ç‰¹å¾å’Œå“åº”å˜é‡ä¹‹é—´å…³ç³»çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•è¡¡é‡çš„æ˜¯å˜é‡ä¹‹é—´çš„çº¿æ€§ç›¸å…³æ€§ï¼Œç»“æœçš„å–å€¼åŒºé—´ä¸º[-1ï¼Œ1]ï¼Œ-1è¡¨ç¤ºå®Œå…¨çš„è´Ÿç›¸å…³ï¼Œ+1è¡¨ç¤ºå®Œå…¨çš„æ­£ç›¸å…³ï¼Œ0è¡¨ç¤ºæ²¡æœ‰çº¿æ€§ç›¸å…³ã€‚

ã€€ã€€Pearson Correlationé€Ÿåº¦å¿«ã€æ˜“äºè®¡ç®—ï¼Œç»å¸¸åœ¨æ‹¿åˆ°æ•°æ®(ç»è¿‡æ¸…æ´—å’Œç‰¹å¾æå–ä¹‹åçš„)ä¹‹åç¬¬ä¸€æ—¶é—´å°±æ‰§è¡Œã€‚Scipyçš„ pearsonr æ–¹æ³•èƒ½å¤ŸåŒæ—¶è®¡ç®— ç›¸å…³ç³»æ•° å’Œp-value.

```python
import numpy as np
from scipy.stats import pearsonr
np.random.seed(0)
size = 300
x = np.random.normal(0, 1, size)
# pearsonr(x, y)çš„è¾“å…¥ä¸ºç‰¹å¾çŸ©é˜µå’Œç›®æ ‡å‘é‡
print("Lower noise", pearsonr(x, x + np.random.normal(0, 1, size)))
print("Higher noise", pearsonr(x, x + np.random.normal(0, 10, size)))
>>>
# è¾“å‡ºä¸ºäºŒå…ƒç»„(sorce, p-value)çš„æ•°ç»„
Lower noise (0.71824836862138386, 7.3240173129992273e-49)
Higher noise (0.057964292079338148, 0.31700993885324746)
```

è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†å˜é‡åœ¨åŠ å…¥å™ªéŸ³ä¹‹å‰å’Œä¹‹åçš„å·®å¼‚ã€‚å½“å™ªéŸ³æ¯”è¾ƒå°çš„æ—¶å€™ï¼Œç›¸å…³æ€§å¾ˆå¼ºï¼Œp-valueå¾ˆä½ã€‚

ã€€ã€€Scikit-learnæä¾›çš„ [f_regrssion](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html) æ–¹æ³•èƒ½å¤Ÿæ‰¹é‡è®¡ç®—ç‰¹å¾çš„f_scoreå’Œp-valueï¼Œéå¸¸æ–¹ä¾¿ï¼Œå‚è€ƒsklearnçš„ [pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)

ã€€ã€€ **Pearsonç›¸å…³ç³»æ•°çš„ä¸€ä¸ªæ˜æ˜¾ç¼ºé™·æ˜¯ï¼Œä½œä¸ºç‰¹å¾æ’åºæœºåˆ¶ï¼Œä»–åªå¯¹çº¿æ€§å…³ç³»æ•æ„Ÿã€‚å¦‚æœå…³ç³»æ˜¯éçº¿æ€§çš„ï¼Œå³ä¾¿ä¸¤ä¸ªå˜é‡å…·æœ‰ä¸€ä¸€å¯¹åº”çš„å…³ç³»ï¼ŒPearsonç›¸å…³æ€§ä¹Ÿå¯èƒ½ä¼šæ¥è¿‘0ã€‚** ä¾‹å¦‚ï¼š

> x = np.random.uniform(-1, 1, 100000)
> print pearsonr(x, x**2)[0]
> -0.00230804707612

ã€€ã€€æ›´å¤šç±»ä¼¼çš„ä¾‹å­å‚è€ƒ [sample plots](http://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Correlation_examples2.svg/506px-Correlation_examples2.svg.png) ã€‚å¦å¤–ï¼Œå¦‚æœä»…ä»…æ ¹æ®ç›¸å…³ç³»æ•°è¿™ä¸ªå€¼æ¥åˆ¤æ–­çš„è¯ï¼Œæœ‰æ—¶å€™ä¼šå…·æœ‰å¾ˆå¼ºçš„è¯¯å¯¼æ€§ï¼Œå¦‚ [Anscombeâ€™s quartet](http://www.matrix67.com/blog/archives/2308) ï¼Œæœ€å¥½æŠŠæ•°æ®å¯è§†åŒ–å‡ºæ¥ï¼Œä»¥å…å¾—å‡ºé”™è¯¯çš„ç»“è®ºã€‚

#### 2.3 äº’ä¿¡æ¯å’Œæœ€å¤§ä¿¡æ¯ç³»æ•° (Mutual information and maximal information coefficient (MIC)

ã€€ã€€ç»å…¸çš„äº’ä¿¡æ¯ï¼ˆäº’ä¿¡æ¯ä¸ºéšæœºå˜é‡Xä¸Yä¹‹é—´çš„äº’ä¿¡æ¯**ğ¼**(**ğ‘‹**;**ğ‘Œ**)I(X;Y)ä¸ºå•ä¸ªäº‹ä»¶ä¹‹é—´äº’ä¿¡æ¯çš„æ•°å­¦æœŸæœ›ï¼‰ä¹Ÿæ˜¯è¯„ä»·å®šæ€§è‡ªå˜é‡å¯¹å®šæ€§å› å˜é‡çš„ç›¸å…³æ€§çš„ï¼Œäº’ä¿¡æ¯è®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š

**ğ¼**(**ğ‘‹**;**ğ‘Œ**)**=**ğ¸**[**ğ¼**(**ğ‘¥**ğ‘–**;**ğ‘¦**ğ‘—**)**]**=**âˆ‘**ğ‘¥**ğ‘–**ğœ–**ğ‘‹**âˆ‘**ğ‘¦**ğ‘—**ğœ–**ğ‘Œ**ğ‘**(**ğ‘¥**ğ‘–**,**ğ‘¦**ğ‘—**)**ğ‘™**ğ‘œ**ğ‘”**ğ‘**(**ğ‘¥**ğ‘–**,**ğ‘¦**ğ‘—**)**ğ‘**(**ğ‘¥**ğ‘–**)**ğ‘**(**ğ‘¦**ğ‘—**)I(X;Y)=E[I(xi;yj)]=âˆ‘xiÏµXâˆ‘yjÏµYp(xi,yj)logp(xi,yj)p(xi)p(yj)

ã€€ã€€äº’ä¿¡æ¯ç›´æ¥ç”¨äºç‰¹å¾é€‰æ‹©å…¶å®ä¸æ˜¯å¤ªæ–¹ä¾¿ï¼š1ã€å®ƒä¸å±äºåº¦é‡æ–¹å¼ï¼Œä¹Ÿæ²¡æœ‰åŠæ³•å½’ä¸€åŒ–ï¼Œåœ¨ä¸åŒæ•°æ®åŠä¸Šçš„ç»“æœæ— æ³•åšæ¯”è¾ƒï¼›2ã€å¯¹äºè¿ç»­å˜é‡çš„è®¡ç®—ä¸æ˜¯å¾ˆæ–¹ä¾¿ï¼ˆXå’ŒYéƒ½æ˜¯é›†åˆï¼Œxï¼Œyéƒ½æ˜¯ç¦»æ•£çš„å–å€¼ï¼‰ï¼Œé€šå¸¸å˜é‡éœ€è¦å…ˆç¦»æ•£åŒ–ï¼Œè€Œäº’ä¿¡æ¯çš„ç»“æœå¯¹ç¦»æ•£åŒ–çš„æ–¹å¼å¾ˆæ•æ„Ÿã€‚

ã€€ã€€æœ€å¤§ä¿¡æ¯ç³»æ•°å…‹æœäº†è¿™ä¸¤ä¸ªé—®é¢˜ã€‚å®ƒé¦–å…ˆå¯»æ‰¾ä¸€ç§æœ€ä¼˜çš„ç¦»æ•£åŒ–æ–¹å¼ï¼Œç„¶åæŠŠäº’ä¿¡æ¯å–å€¼è½¬æ¢æˆä¸€ç§åº¦é‡æ–¹å¼ï¼Œå–å€¼åŒºé—´åœ¨[0ï¼Œ1]ã€‚ [minepy](http://minepy.readthedocs.io/en/latest/) æä¾›äº†MICåŠŸèƒ½ã€‚

åè¿‡å¤´æ¥çœ‹**ğ‘¦**=**ğ‘¥**2y=x2è¿™ä¸ªä¾‹å­ï¼ŒMICç®—å‡ºæ¥çš„äº’ä¿¡æ¯å€¼ä¸º1(æœ€å¤§çš„å–å€¼)ã€‚

```python
from minepy import MINE
m = MINE()
x = np.random.uniform(-1, 1, 10000)
m.compute_score(x, x**2)
print(m.mic())
>>>1.0
```

ã€€ã€€MICçš„ç»Ÿè®¡èƒ½åŠ›é­åˆ°äº† [ä¸€äº›è´¨ç–‘](http://statweb.stanford.edu/~tibs/reshef/comment.pdf) ï¼Œå½“é›¶å‡è®¾ä¸æˆç«‹æ—¶ï¼ŒMICçš„ç»Ÿè®¡å°±ä¼šå—åˆ°å½±å“ã€‚åœ¨æœ‰çš„æ•°æ®é›†ä¸Šä¸å­˜åœ¨è¿™ä¸ªé—®é¢˜ï¼Œä½†æœ‰çš„æ•°æ®é›†ä¸Šå°±å­˜åœ¨è¿™ä¸ªé—®é¢˜ã€‚

#### 2.4 è·ç¦»ç›¸å…³ç³»æ•° (Distance Correlation)

ã€€ã€€è·ç¦»ç›¸å…³ç³»æ•°æ˜¯ä¸ºäº†å…‹æœPearsonç›¸å…³ç³»æ•°çš„å¼±ç‚¹è€Œç”Ÿçš„ã€‚åœ¨**ğ‘¥**xå’Œ**ğ‘¥**2x2è¿™ä¸ªä¾‹å­ä¸­ï¼Œå³ä¾¿Pearsonç›¸å…³ç³»æ•°æ˜¯0ï¼Œæˆ‘ä»¬ä¹Ÿä¸èƒ½æ–­å®šè¿™ä¸¤ä¸ªå˜é‡æ˜¯ç‹¬ç«‹çš„ï¼ˆæœ‰å¯èƒ½æ˜¯éçº¿æ€§ç›¸å…³ï¼‰ï¼›ä½†å¦‚æœè·ç¦»ç›¸å…³ç³»æ•°æ˜¯0ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±å¯ä»¥è¯´è¿™ä¸¤ä¸ªå˜é‡æ˜¯ç‹¬ç«‹çš„ã€‚

ã€€ã€€Rçš„ energy åŒ…é‡Œæä¾›äº†è·ç¦»ç›¸å…³ç³»æ•°çš„å®ç°ï¼Œå¦å¤–è¿™æ˜¯ [Python gist](https://gist.github.com/josef-pkt/2938402) çš„å®ç°ã€‚

```r
> x = runif (1000, -1, 1)
> dcor(x, x**2)
[1] 0.4943864
```

ã€€ã€€å°½ç®¡æœ‰ MIC å’Œ è·ç¦»ç›¸å…³ç³»æ•° åœ¨äº†ï¼Œä½†å½“å˜é‡ä¹‹é—´çš„å…³ç³»æ¥è¿‘çº¿æ€§ç›¸å…³çš„æ—¶å€™ï¼ŒPearsonç›¸å…³ç³»æ•°ä»ç„¶æ˜¯ä¸å¯æ›¿ä»£çš„ã€‚
ã€€ã€€ç¬¬ä¸€ï¼ŒPearsonç›¸å…³ç³»æ•°è®¡ç®—é€Ÿåº¦å¿«ï¼Œè¿™åœ¨å¤„ç†å¤§è§„æ¨¡æ•°æ®çš„æ—¶å€™å¾ˆé‡è¦ã€‚
ã€€ã€€ç¬¬äºŒï¼ŒPearsonç›¸å…³ç³»æ•°çš„å–å€¼åŒºé—´æ˜¯[-1ï¼Œ1]ï¼Œè€ŒMICå’Œè·ç¦»ç›¸å…³ç³»æ•°éƒ½æ˜¯[0ï¼Œ1]ã€‚è¿™ä¸ªç‰¹ç‚¹ä½¿å¾—Pearsonç›¸å…³ç³»æ•°èƒ½å¤Ÿè¡¨å¾æ›´ä¸°å¯Œçš„å…³ç³»ï¼Œç¬¦å·è¡¨ç¤ºå…³ç³»çš„æ­£è´Ÿï¼Œç»å¯¹å€¼èƒ½å¤Ÿè¡¨ç¤ºå¼ºåº¦ã€‚å½“ç„¶ï¼ŒPearsonç›¸å…³æ€§æœ‰æ•ˆçš„å‰ææ˜¯ä¸¤ä¸ªå˜é‡çš„å˜åŒ–å…³ç³»æ˜¯å•è°ƒçš„ã€‚

#### 2.5 åŸºäºæ¨¡å‹çš„ç‰¹å¾æ’åº (Model based ranking)

ã€€ã€€è¿™ç§æ–¹æ³•çš„æ€è·¯æ˜¯ç›´æ¥ä½¿ç”¨ä½ è¦ç”¨çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œ **é’ˆå¯¹ æ¯ä¸ªå•ç‹¬çš„ç‰¹å¾ å’Œ å“åº”å˜é‡å»ºç«‹é¢„æµ‹æ¨¡å‹ã€‚** å‡å¦‚ ç‰¹å¾ å’Œ å“åº”å˜é‡ ä¹‹é—´çš„å…³ç³»æ˜¯ **éçº¿æ€§çš„** ï¼Œå¯ä»¥ç”¨åŸºäºæ ‘çš„æ–¹æ³•(å†³ç­–æ ‘ã€éšæœºæ£®æ—)ã€æˆ–è€… æ‰©å±•çš„çº¿æ€§æ¨¡å‹ ç­‰ã€‚åŸºäºæ ‘çš„æ–¹æ³•æ¯”è¾ƒæ˜“äºä½¿ç”¨ï¼Œå› ä¸ºä»–ä»¬å¯¹éçº¿æ€§å…³ç³»çš„å»ºæ¨¡æ¯”è¾ƒå¥½ï¼Œå¹¶ä¸”ä¸éœ€è¦å¤ªå¤šçš„è°ƒè¯•ã€‚ä½†è¦æ³¨æ„è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå› æ­¤æ ‘çš„æ·±åº¦æœ€å¥½ä¸è¦å¤ªå¤§ï¼Œå†å°±æ˜¯ **è¿ç”¨äº¤å‰éªŒè¯** ã€‚

ã€€ã€€åœ¨ æ³¢å£«é¡¿æˆ¿ä»·æ•°æ®é›† ä¸Šä½¿ç”¨sklearnçš„ éšæœºæ£®æ—å›å½’ ç»™å‡ºä¸€ä¸ª_å•å˜é‡é€‰æ‹©_çš„ä¾‹å­(è¿™é‡Œä½¿ç”¨äº†äº¤å‰éªŒè¯)ï¼š

```python
from sklearn.cross_validation import cross_val_score, ShuffleSplit
from sklearn.datasets import load_boston
from sklearn.ensemble import RandomForestRegressor
import numpy as np

# Load boston housing dataset as an example
boston = load_boston()
X = boston["data"]
Y = boston["target"]
names = boston["feature_names"]

rf = RandomForestRegressor(n_estimators=20, max_depth=4)
scores = []
# å•ç‹¬é‡‡ç”¨æ¯ä¸ªç‰¹å¾è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶è¿›è¡Œäº¤å‰éªŒè¯
for i in range(X.shape[1]):
    score = cross_val_score(rf, X[:, i:i+1], Y, scoring="r2",  # æ³¨æ„X[:, i]å’ŒX[:, i:i+1]çš„åŒºåˆ«
                            cv=ShuffleSplit(len(X), 3, .3))
    scores.append((format(np.mean(score), '.3f'), names[i]))
print(sorted(scores, reverse=True))
```

> [('0.620', 'LSTAT'), ('0.591', 'RM'), ('0.467', 'NOX'), ('0.342', 'INDUS'), ('0.305', 'TAX'), ('0.240', 'PTRATIO'), ('0.206', 'CRIM'), ('0.187', 'RAD'), ('0.184', 'ZN'), ('0.135', 'B'), ('0.082', 'DIS'), ('0.020', 'CHAS'), ('0.002', 'AGE')]

## Wrapper

### 3. é€’å½’ç‰¹å¾æ¶ˆé™¤ (Recursive Feature Elimination)

ã€€ã€€é€’å½’æ¶ˆé™¤ç‰¹å¾æ³• **ä½¿ç”¨ä¸€ä¸ªåŸºæ¨¡å‹æ¥è¿›è¡Œå¤šè½®è®­ç»ƒï¼Œæ¯è½®è®­ç»ƒåï¼Œç§»é™¤è‹¥å¹²æƒå€¼ç³»æ•°çš„ç‰¹å¾ï¼Œå†åŸºäºæ–°çš„ç‰¹å¾é›†è¿›è¡Œä¸‹ä¸€è½®è®­ç»ƒ** ã€‚

ã€€ã€€ **sklearnå®˜æ–¹è§£é‡Š** ï¼šå¯¹ç‰¹å¾å«æœ‰æƒé‡çš„é¢„æµ‹æ¨¡å‹(ä¾‹å¦‚ï¼Œçº¿æ€§æ¨¡å‹å¯¹åº”å‚æ•°coefficients)ï¼ŒRFEé€šè¿‡ **é€’å½’å‡å°‘è€ƒå¯Ÿçš„ç‰¹å¾é›†è§„æ¨¡æ¥é€‰æ‹©ç‰¹å¾** ã€‚é¦–å…ˆï¼Œé¢„æµ‹æ¨¡å‹åœ¨åŸå§‹ç‰¹å¾ä¸Šè®­ç»ƒï¼Œæ¯ä¸ªç‰¹å¾æŒ‡å®šä¸€ä¸ªæƒé‡ã€‚ä¹‹åï¼Œé‚£äº›æ‹¥æœ‰æœ€å°ç»å¯¹å€¼æƒé‡çš„ç‰¹å¾è¢«è¸¢å‡ºç‰¹å¾é›†ã€‚å¦‚æ­¤å¾€å¤é€’å½’ï¼Œç›´è‡³å‰©ä½™çš„ç‰¹å¾æ•°é‡è¾¾åˆ°æ‰€éœ€çš„ç‰¹å¾æ•°é‡ã€‚

ã€€ã€€RFECV é€šè¿‡äº¤å‰éªŒè¯çš„æ–¹å¼æ‰§è¡ŒRFEï¼Œä»¥æ­¤æ¥é€‰æ‹©æœ€ä½³æ•°é‡çš„ç‰¹å¾ï¼šå¯¹äºä¸€ä¸ªæ•°é‡ä¸ºdçš„featureçš„é›†åˆï¼Œä»–çš„æ‰€æœ‰çš„å­é›†çš„ä¸ªæ•°æ˜¯2çš„dæ¬¡æ–¹å‡1(åŒ…å«ç©ºé›†)ã€‚æŒ‡å®šä¸€ä¸ªå¤–éƒ¨çš„å­¦ä¹ ç®—æ³•ï¼Œæ¯”å¦‚SVMä¹‹ç±»çš„ã€‚é€šè¿‡è¯¥ç®—æ³•è®¡ç®—æ‰€æœ‰å­é›†çš„validation errorã€‚é€‰æ‹©erroræœ€å°çš„é‚£ä¸ªå­é›†ä½œä¸ºæ‰€æŒ‘é€‰çš„ç‰¹å¾ã€‚

```python
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

#é€’å½’ç‰¹å¾æ¶ˆé™¤æ³•ï¼Œè¿”å›ç‰¹å¾é€‰æ‹©åçš„æ•°æ®
#å‚æ•°estimatorä¸ºåŸºæ¨¡å‹
#å‚æ•°n_features_to_selectä¸ºé€‰æ‹©çš„ç‰¹å¾ä¸ªæ•°
RFE(estimator=LogisticRegression(), n_features_to_select=2).fit_transform(iris.data, iris.target)
```

> ç¤ºä¾‹:
> [Recursive feature elimination](http://sklearn.lzjqsdd.com/auto_examples/feature_selection/plot_rfe_digits.html#example-feature-selection-plot-rfe-digits-py): ä¸€ä¸ªé€’å½’ç‰¹å¾æ¶ˆé™¤çš„ç¤ºä¾‹ï¼Œå±•ç¤ºäº†åœ¨æ•°å­—åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œåƒç´ ä¹‹é—´çš„ç›¸å…³æ€§ã€‚
> [Recursive feature elimination with cross-validation](http://sklearn.lzjqsdd.com/auto_examples/feature_selection/plot_rfe_with_cross_validation.html#example-feature-selection-plot-rfe-with-cross-validation-py): ä¸€ä¸ªé€’å½’ç‰¹å¾æ¶ˆé™¤ç¤ºä¾‹ï¼Œé€šè¿‡äº¤å‰éªŒè¯çš„æ–¹å¼è‡ªåŠ¨è°ƒæ•´æ‰€é€‰ç‰¹å¾çš„æ•°é‡ã€‚

## Embedded

### 4. ä½¿ç”¨SelectFromModelé€‰æ‹©ç‰¹å¾ (Feature selection using SelectFromModel)

ã€€ã€€å•å˜é‡ç‰¹å¾é€‰æ‹©æ–¹æ³•ç‹¬ç«‹çš„è¡¡é‡æ¯ä¸ªç‰¹å¾ä¸å“åº”å˜é‡ä¹‹é—´çš„å…³ç³»ï¼Œå¦ä¸€ç§ä¸»æµçš„ç‰¹å¾é€‰æ‹©æ–¹æ³•æ˜¯åŸºäºæœºå™¨å­¦ä¹ æ¨¡å‹çš„æ–¹æ³•ã€‚æœ‰äº›æœºå™¨å­¦ä¹ æ–¹æ³•æœ¬èº«å°±å…·æœ‰å¯¹ç‰¹å¾è¿›è¡Œæ‰“åˆ†çš„æœºåˆ¶ï¼Œæˆ–è€…å¾ˆå®¹æ˜“å°†å…¶è¿ç”¨åˆ°ç‰¹å¾é€‰æ‹©ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚å›å½’æ¨¡å‹ï¼ŒSVMï¼Œå†³ç­–æ ‘ï¼Œéšæœºæ£®æ—ç­‰ç­‰ã€‚å…¶å®Pearsonç›¸å…³ç³»æ•°ç­‰ä»·äºçº¿æ€§å›å½’é‡Œçš„æ ‡å‡†åŒ–å›å½’ç³»æ•°ã€‚

ã€€ã€€SelectFromModel ä½œä¸ºmeta-transformerï¼Œèƒ½å¤Ÿç”¨äºæ‹Ÿåˆåä»»ä½•æ‹¥æœ‰ `coef_`æˆ– `feature_importances_` å±æ€§çš„é¢„æµ‹æ¨¡å‹ã€‚ å¦‚æœç‰¹å¾å¯¹åº”çš„ `coef_` æˆ– `feature_importances_` å€¼ä½äºè®¾å®šçš„é˜ˆå€¼ `threshold`ï¼Œé‚£ä¹ˆè¿™äº›ç‰¹å¾å°†è¢«ç§»é™¤ã€‚é™¤äº†æ‰‹åŠ¨è®¾ç½®é˜ˆå€¼ï¼Œä¹Ÿå¯é€šè¿‡å­—ç¬¦ä¸²å‚æ•°è°ƒç”¨å†…ç½®çš„å¯å‘å¼ç®—æ³•(heuristics)æ¥è®¾ç½®é˜ˆå€¼ï¼ŒåŒ…æ‹¬ï¼šå¹³å‡å€¼(â€œmeanâ€), ä¸­ä½æ•°(â€œmedianâ€)ä»¥åŠä»–ä»¬ä¸æµ®ç‚¹æ•°çš„ä¹˜ç§¯ï¼Œå¦‚â€0.1*meanâ€ã€‚

> Examples
> [Feature selection using SelectFromModel and LassoCV](http://sklearn.lzjqsdd.com/auto_examples/feature_selection/plot_select_from_model_boston.html#example-feature-selection-plot-select-from-model-boston-py): åœ¨é˜ˆå€¼æœªçŸ¥çš„å‰æä¸‹ï¼Œé€‰æ‹©äº†Boston datasetä¸­ä¸¤é¡¹æœ€é‡è¦çš„ç‰¹å¾ã€‚

#### 4.1 åŸºäºL1çš„ç‰¹å¾é€‰æ‹© (L1-based feature selection)

ã€€ã€€ä½¿ç”¨L1èŒƒæ•°ä½œä¸ºæƒ©ç½šé¡¹çš„çº¿æ€§æ¨¡å‹(Linear models)ä¼šå¾—åˆ°ç¨€ç–è§£ï¼šå¤§éƒ¨åˆ†ç‰¹å¾å¯¹åº”çš„ç³»æ•°ä¸º0ã€‚å½“ä½ å¸Œæœ›å‡å°‘ç‰¹å¾çš„ç»´åº¦ä»¥ç”¨äºå…¶å®ƒåˆ†ç±»å™¨æ—¶ï¼Œå¯ä»¥é€šè¿‡ `feature_selection.SelectFromModel` æ¥é€‰æ‹©ä¸ä¸º0çš„ç³»æ•°ã€‚ç‰¹åˆ«æŒ‡å‡ºï¼Œå¸¸ç”¨äºæ­¤ç›®çš„çš„ç¨€ç–é¢„æµ‹æ¨¡å‹æœ‰ `linear_model.Lasso`ï¼ˆå›å½’ï¼‰ï¼Œ linear_model.LogisticRegression å’Œ svm.LinearSVCï¼ˆåˆ†ç±»ï¼‰:

```python
>>> from sklearn.svm import LinearSVC
>>> from sklearn.datasets import load_iris
>>> from sklearn.feature_selection import SelectFromModel
>>> iris = load_iris()
>>> X, y = iris.data, iris.target
>>> X.shape
(150, 4)
>>> lsvc = LinearSVC(C=0.01, penalty="l1", dual=False).fit(X, y)
>>> model = SelectFromModel(lsvc, prefit=True)
>>> X_new = model.transform(X)
>>> X_new.shape
(150, 3)
```

ã€€ã€€ä½¿ç”¨feature_selectionåº“çš„SelectFromModelç±»ç»“åˆå¸¦L1ä»¥åŠL2æƒ©ç½šé¡¹çš„é€»è¾‘å›å½’æ¨¡å‹:

```python
from sklearn.feature_selection import SelectFromModel
#å¸¦L1å’ŒL2æƒ©ç½šé¡¹çš„é€»è¾‘å›å½’ä½œä¸ºåŸºæ¨¡å‹çš„ç‰¹å¾é€‰æ‹©
#å‚æ•°thresholdä¸ºæƒå€¼ç³»æ•°ä¹‹å·®çš„é˜ˆå€¼
SelectFromModel(LR(threshold=0.5, C=0.1)).fit_transform(iris.data, iris.target)
```

ã€€ã€€_å¯¹äºSVMå’Œé€»è¾‘å›å½’ï¼Œå‚æ•°Cæ§åˆ¶ç¨€ç–æ€§ï¼šCè¶Šå°ï¼Œè¢«é€‰ä¸­çš„ç‰¹å¾è¶Šå°‘ã€‚å¯¹äºLassoï¼Œå‚æ•°alphaè¶Šå¤§ï¼Œè¢«é€‰ä¸­çš„ç‰¹å¾è¶Šå°‘ã€‚_

> ç¤ºä¾‹:
> [Classification of text documents using sparse features](http://sklearn.lzjqsdd.com/auto_examples/text/document_classification_20newsgroups.html#example-text-document-classification-20newsgroups-py): ä¸åŒç®—æ³•ä½¿ç”¨åŸºäºL1çš„ç‰¹å¾é€‰æ‹©è¿›è¡Œæ–‡æ¡£åˆ†ç±»çš„å¯¹æ¯”ã€‚

Note:

> L1æ¢å¤å’Œå‹ç¼©æ„ŸçŸ¥ (L1-recovery and compressive sensing)
> ã€€ã€€å¯¹äºä¸€ä¸ªå¥½çš„alphaå€¼ï¼Œåœ¨æ»¡è¶³ç‰¹å®šæ¡ä»¶ä¸‹ï¼Œ Lasso ä»…ä½¿ç”¨å°‘é‡è§‚æµ‹å€¼å°±èƒ½å¤Ÿå®Œå…¨æ¢å¤å‡ºéé›¶çš„ç³»æ•°ã€‚ *ç‰¹åˆ«åœ°ï¼Œæ ·æœ¬çš„æ•°é‡éœ€è¦â€œè¶³å¤Ÿå¤§â€ï¼Œå¦åˆ™L1æ¨¡å‹çš„è¡¨ç°ä¼šå……æ»¡éšæœºæ€§ï¼Œæ‰€è°“â€œè¶³å¤Ÿå¤§â€å–å†³äºéé›¶ç³»æ•°çš„æ•°é‡ï¼Œç‰¹å¾æ•°é‡çš„å¯¹æ•°ï¼Œå™ªå£°çš„æ•°é‡ï¼Œéé›¶ç³»æ•°çš„æœ€å°ç»å¯¹å€¼ä»¥åŠè®¾è®¡çŸ©é˜µXçš„ç»“æ„* ã€‚æ­¤å¤–ï¼Œè®¾è®¡çŸ©é˜µå¿…é¡»æ‹¥æœ‰ç‰¹å®šçš„å±æ€§ï¼Œæ¯”å¦‚ä¸èƒ½å¤ªè¿‡ç›¸å…³(correlated)ã€‚ å¯¹äºéé›¶ç³»æ•°çš„æ¢å¤ï¼Œè¿˜æ²¡æœ‰ä¸€ä¸ªé€‰æ‹©alphaå€¼çš„é€šç”¨è§„åˆ™ ã€‚alphaå€¼å¯ä»¥é€šè¿‡äº¤å‰éªŒè¯æ¥è®¾ç½®(LassoCV or LassoLarsCV)ï¼Œå°½ç®¡è¿™ä¹Ÿè®¸ä¼šå¯¼è‡´æ¨¡å‹æ¬ æƒ©ç½š(under-penalized)ï¼šå¼•å…¥å°‘é‡éç›¸å…³å˜é‡ä¸ä¼šå½±å“åˆ†æ•°é¢„æµ‹ã€‚ç›¸åBIC (LassoLarsIC) æ›´å€¾å‘äºè®¾ç½®è¾ƒå¤§çš„alphaå€¼ã€‚
> [Reference Richard G. Baraniuk â€œCompressive Sensingâ€, IEEE Signal Processing Magazine [120] July 2007](http://dsp.rice.edu/files/cs/baraniukCSlecture07.pdf)

#### 4.2 éšæœºç¨€ç–æ¨¡å‹ (Randomized sparse models)

ã€€ã€€ **åŸºäºL1çš„ç¨€ç–æ¨¡å‹çš„å±€é™åœ¨äºï¼Œå½“é¢å¯¹ä¸€ç»„äº’ç›¸å…³çš„ç‰¹å¾æ—¶ï¼Œå®ƒä»¬åªä¼šé€‰æ‹©å…¶ä¸­ä¸€é¡¹ç‰¹å¾ã€‚ä¸ºäº†å‡è½»è¯¥é—®é¢˜çš„å½±å“å¯ä»¥ä½¿ç”¨éšæœºåŒ–æŠ€æœ¯ï¼Œé€šè¿‡_å¤šæ¬¡é‡æ–°ä¼°è®¡ç¨€ç–æ¨¡å‹æ¥æ‰°ä¹±è®¾è®¡çŸ©é˜µ_ï¼Œæˆ–é€šè¿‡_å¤šæ¬¡ä¸‹é‡‡æ ·æ•°æ®æ¥ç»Ÿè®¡ä¸€ä¸ªç»™å®šçš„å›å½’é‡è¢«é€‰ä¸­çš„æ¬¡æ•°_ã€‚** â€”â€”=ç¨³å®šæ€§é€‰æ‹© (Stability Selection)=

ã€€ã€€RandomizedLasso å®ç°äº†ä½¿ç”¨è¿™é¡¹ç­–ç•¥çš„Lassoï¼ŒRandomizedLogisticRegression ä½¿ç”¨é€»è¾‘å›å½’ï¼Œé€‚ç”¨äºåˆ†ç±»ä»»åŠ¡ã€‚è¦å¾—åˆ°æ•´ä¸ªè¿­ä»£è¿‡ç¨‹çš„ç¨³å®šåˆ†æ•°ï¼Œä½ å¯ä»¥ä½¿ç”¨ `lasso_stability_path`ã€‚

ã€€ã€€æ³¨æ„åˆ°å¯¹äºéé›¶ç‰¹å¾çš„æ£€æµ‹ï¼Œè¦ä½¿éšæœºç¨€ç–æ¨¡å‹æ¯”æ ‡å‡†Fç»Ÿè®¡é‡æ›´æœ‰æ•ˆï¼Œ é‚£ä¹ˆæ¨¡å‹çš„å‚è€ƒæ ‡å‡†éœ€è¦æ˜¯ç¨€ç–çš„ï¼Œæ¢å¥è¯è¯´ï¼Œéé›¶ç‰¹å¾åº”å½“åªå ä¸€å°éƒ¨åˆ†ã€‚

> ç¤ºä¾‹:
> [Sparse recovery: feature selection for sparse linear models](http://sklearn.lzjqsdd.com/auto_examples/linear_model/plot_sparse_recovery.html#example-linear-model-plot-sparse-recovery-py): æ¯”è¾ƒäº†ä¸åŒçš„ç‰¹å¾é€‰æ‹©æ–¹æ³•ï¼Œå¹¶è®¨è®ºäº†å®ƒä»¬å„è‡ªé€‚ç”¨çš„åœºåˆã€‚

> å‚è€ƒæ–‡çŒ®:
> [N. Meinshausen, P. Buhlmann, â€œStability selectionâ€, Journal of the Royal Statistical Society, 72 (2010)](http://arxiv.org/pdf/0809.2932)
> F. [Bach, â€œModel-Consistent Sparse Estimation through the Bootstrapâ€](http://hal.inria.fr/hal-00354771/)

#### 4.3 åŸºäºæ ‘çš„ç‰¹å¾é€‰æ‹© (Tree-based feature selection)

ã€€ã€€åŸºäºæ ‘çš„é¢„æµ‹æ¨¡å‹ï¼ˆè§ sklearn.tree æ¨¡å—ï¼Œæ£®æ—è§ sklearn.ensemble æ¨¡å—ï¼‰èƒ½å¤Ÿç”¨æ¥è®¡ç®—ç‰¹å¾çš„é‡è¦ç¨‹åº¦ï¼Œå› æ­¤èƒ½ç”¨æ¥å»é™¤ä¸ç›¸å…³çš„ç‰¹å¾ï¼ˆç»“åˆ `sklearn.feature_selection.SelectFromModel`ï¼‰:

```python
>>> from sklearn.ensemble import ExtraTreesClassifier
>>> from sklearn.datasets import load_iris
>>> from sklearn.feature_selection import SelectFromModel
>>> iris = load_iris()
>>> X, y = iris.data, iris.target
>>> X.shape
(150, 4)
>>> clf = ExtraTreesClassifier()
>>> clf = clf.fit(X, y)
>>> clf.feature_importances_  
array([ 0.04...,  0.05...,  0.4...,  0.4...])
>>> model = SelectFromModel(clf, prefit=True)
>>> X_new = model.transform(X)
>>> X_new.shape             
(150, 2)
```

> ç¤ºä¾‹:
> [Feature importances with forests of trees](http://sklearn.lzjqsdd.com/auto_examples/ensemble/plot_forest_importances.html#example-ensemble-plot-forest-importances-py): ä»æ¨¡æ‹Ÿæ•°æ®ä¸­æ¢å¤æœ‰æ„ä¹‰çš„ç‰¹å¾ã€‚
> [Pixel importances with a parallel forest of trees](http://sklearn.lzjqsdd.com/auto_examples/ensemble/plot_forest_importances_faces.html#example-ensemble-plot-forest-importances-faces-py): ç”¨äºäººè„¸è¯†åˆ«æ•°æ®çš„ç¤ºä¾‹ã€‚

### 5. å°†ç‰¹å¾é€‰æ‹©è¿‡ç¨‹èå…¥pipeline (Feature selection as part of a pipeline)

ã€€ã€€ç‰¹å¾é€‰æ‹©å¸¸å¸¸è¢«å½“ä½œå­¦ä¹ ä¹‹å‰çš„ä¸€é¡¹é¢„å¤„ç†ã€‚åœ¨scikit-learnä¸­æ¨èä½¿ç”¨
`sklearn.pipeline.Pipeline`:

```python
clf = Pipeline([
  ('feature_selection', SelectFromModel(LinearSVC(penalty="l1"))),
  ('classification', RandomForestClassifier())
])
clf.fit(X, y)
```

ã€€ã€€åœ¨æ­¤ä»£ç ç‰‡æ®µä¸­ï¼Œå°†ã€€sklearn.svm.LinearSVC å’Œ sklearn.feature_selection.SelectFromModel ç»“åˆæ¥è¯„ä¼°ç‰¹å¾çš„é‡è¦æ€§ï¼Œå¹¶é€‰æ‹©æœ€ç›¸å…³çš„ç‰¹å¾ã€‚ä¹‹å sklearn.ensemble.RandomForestClassifier æ¨¡å‹ä½¿ç”¨è½¬æ¢åçš„è¾“å‡ºè®­ç»ƒï¼Œå³åªä½¿ç”¨è¢«é€‰å‡ºçš„ç›¸å…³ç‰¹å¾ã€‚ä½ å¯ä»¥é€‰æ‹©å…¶å®ƒç‰¹å¾é€‰æ‹©æ–¹æ³•ï¼Œæˆ–æ˜¯å…¶å®ƒæä¾›ç‰¹å¾é‡è¦æ€§è¯„ä¼°çš„åˆ†ç±»å™¨ã€‚æ›´å¤šè¯¦æƒ…è§ [sklearn.pipeline.Pipeline](http://sklearn.lzjqsdd.com/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline) ç›¸å…³ç¤ºä¾‹ã€‚
ã€€ã€€
***å…³äºæ›´å¤šï¼Œå‚è§å¦ä¸€ä¸ªæ–‡æ¡£ï¼š
ã€ŠåŸºäºæ¨¡å‹çš„ç‰¹å¾é€‰æ‹©è¯¦è§£ (Embedded & Wrapper)ã€‹***

---

**å°ç»“ï¼š**

| ç±»                | æ‰€å±æ–¹å¼ | è¯´æ˜                                                   |
| ----------------- | -------- | ------------------------------------------------------ |
| VarianceThreshold | Filter   | æ–¹å·®é€‰æ‹©æ³•(ç§»é™¤ä½æ–¹å·®çš„ç‰¹å¾)                           |
| SelectKBest       | Filter   | å¯é€‰å…³è”ç³»æ•°ã€å¡æ–¹æ ¡éªŒã€æœ€å¤§ä¿¡æ¯ç³»æ•°ä½œä¸ºå¾—åˆ†è®¡ç®—çš„æ–¹æ³• |
| RFE               | Wrapper  | é€’å½’åœ°è®­ç»ƒåŸºæ¨¡å‹ï¼Œå°†æƒå€¼ç³»æ•°è¾ƒå°çš„ç‰¹å¾ä»ç‰¹å¾é›†åˆä¸­æ¶ˆé™¤ |
| SelectFromModel   | Embedded | è®­ç»ƒåŸºæ¨¡å‹ï¼Œé€‰æ‹©æƒå€¼ç³»æ•°è¾ƒé«˜çš„ç‰¹å¾                     |

---

**å‚è€ƒï¼š**
[1] [1.13. Feature selection](http://scikit-learn.org/stable/modules/feature_selection.html#feature-selection)
[2] [1.13 ç‰¹å¾é€‰æ‹©](http://sklearn.lzjqsdd.com/modules/feature_selection.html#feature-selection)
[3] [å¹²è´§ï¼šç»“åˆScikit-learnä»‹ç»å‡ ç§å¸¸ç”¨çš„ç‰¹å¾é€‰æ‹©æ–¹æ³•](http://www.tuicool.com/articles/ieUvaq)
[4] [ä½¿ç”¨sklearnåšå•æœºç‰¹å¾å·¥ç¨‹](http://www.cnblogs.com/jasonfreak/p/5448385.html#3601031)
[5] [**ä½¿ç”¨sklearnä¼˜é›…åœ°è¿›è¡Œæ•°æ®æŒ–æ˜**](http://www.cnblogs.com/jasonfreak/p/5448462.html)
[6] [è°åŠ¨äº†æˆ‘çš„ç‰¹å¾ï¼Ÿâ€”â€”sklearnç‰¹å¾è½¬æ¢è¡Œä¸ºå…¨è®°å½•](http://www.cnblogs.com/jasonfreak/p/5619260.html)

**æ³¨ï¼š**
ã€€ã€€**æ–‡æ¡£[4]å®é™…ä¸Šæ˜¯ç”¨sklearnå®ç°æ•´ä¸ªæ•°æ®æŒ–æ˜æµç¨‹ï¼Œç‰¹åˆ«æ˜¯åœ¨æé«˜æ•ˆç‡ä¸Šsklearnçš„å¹¶è¡Œå¤„ç†ï¼Œæµæ°´çº¿å¤„ç†ï¼Œè‡ªåŠ¨åŒ–è°ƒå‚ï¼ŒæŒä¹…åŒ–æ˜¯ä½¿ç”¨sklearnä¼˜é›…åœ°è¿›è¡Œæ•°æ®æŒ–æ˜çš„æ ¸å¿ƒã€‚è¿™é‡Œæ˜¯ä¸€ä¸ªç®€å•çš„æ€»ç»“ï¼Œå…·ä½“å¯æŸ¥çœ‹è¯¥æ–‡æ¡£ï¼š**

| åŒ…                  | ç±»æˆ–æ–¹æ³•     | è¯´æ˜                       |
| ------------------- | ------------ | -------------------------- |
| sklearn.pipeline    | Pipeline     | æµæ°´çº¿å¤„ç†                 |
| sklearn.pipeline    | FeatureUnion | å¹¶è¡Œå¤„ç†                   |
| sklearn.grid_search | GridSearchCV | ç½‘æ ¼æœç´¢è‡ªåŠ¨åŒ–è°ƒå‚         |
| externals.joblib    | dump         | æ•°æ®æŒä¹…åŒ–                 |
| externals.joblib    | load         | ä»æ–‡ä»¶ç³»ç»Ÿä¸­åŠ è½½æ•°æ®è‡³å†…å­˜ |
