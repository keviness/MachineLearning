# 特征筛选（随机森林）

随机森林能够度量每个特征的重要性，我们可以依据这个重要性指标进而选择最重要的特征。sklearn中已经实现了用随机森林评估特征重要性，在训练好随机森林模型后，直接调用feature_importan[ces](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.aibbt.com%2Fa%2Ftag%2Fces%2F)属性就能得到每个特征的重要性。

# 特征筛选（随机森林）

一般情况下，数据集的特征成百上千，因此有必要从中选取对结果影响较大的特征来进行进一步建模，相关的方法有：主成分分析、lasso等，这里我们介绍的是通过随机森林来进行筛选。

用随机森林进行特征重要性评估的思想比较简单，主要是看每个特征在随机森林中的每棵树上做了多大的贡献，然后取平均值，最后比较不同特征之间的贡献大小。

贡献度的衡量指标包括：基尼指数（gini）、袋外数据（OOB）错误率作为评价指标来衡量。

**衍生知识点：权重随机森林的应用（用于增加小样本的识别概率，从而提高总体的分类准确率）**

随机森林/CART树在使用时一般通过gini值作为切分节点的标准，而在加权随机森林（WRF）中，权重的本质是赋给小类较大的权重，给大类较小的权重。也就是给小类更大的惩罚。权重的作用有2个，第1点是用于切分点选择中加权计算gini值，表达式如下：

![](https://upload-images.jianshu.io/upload_images/11576306-34084a238402b4ec.png?imageMogr2/auto-orient/strip|imageView2/2/w/336/format/webp)

image

其中，N表示未分离的节点，N~L~和N~R~分别表示分离后的左侧节点和右侧节点，W~i~为c类样本的类权重，n~i~表示节点内各类样本的数量，Δi是不纯度减少量，该值越大表明分离点的分离效果越好。

第2点是在终节点，类权重用来决定其类标签，表达式如下：

![](https://upload-images.jianshu.io/upload_images/11576306-b0b9b4a12aec1b51.png?imageMogr2/auto-orient/strip|imageView2/2/w/697/format/webp)

image

参考文献：随机森林针对小样本数据类权重设置 [https://wenku.baidu.com/view/07ba98cca0c7aa00b52acfc789eb172ded639998.html](https://links.jianshu.com/go?to=https%3A%2F%2Fwenku.baidu.com%2Fview%2F07ba98cca0c7aa00b52acfc789eb172ded639998.html)

这里介绍通过gini值来进行评价，我们将变量的重要性评分用VIM来表示，gini值用GI表示，假设有m个特征X ~1~ ，X ~2~ ，...X ~c~ ，现在要计算出每个特征X~j~的gini指数评分VIM ~j~ ，即第j个特征在随机森林所有决策树中节点分裂不纯度的平均改变量，gini指数的计算公式如下表示：

![](https://upload-images.jianshu.io/upload_images/11576306-f60404c6eea0afe2.png?imageMogr2/auto-orient/strip|imageView2/2/w/337/format/webp)

image

其中，k表示有k个类别，p~mk~表示节点m（将特征m逐个对节点计算gini值变化量）中类别k所占的比例。

特征X~j~在节点m的重要性，即节点m分枝前后的gini指数变化量为：

![](https://upload-images.jianshu.io/upload_images/11576306-3385fc64a9baef8e.png?imageMogr2/auto-orient/strip|imageView2/2/w/456/format/webp)

image

其中GI~l~和GI~r~分别表示分枝后两个新节点的gini指数。

如果特征X~j~在决策树i中出现的节点在集合M中，那么X~j~在第i棵树的重要性为：

![](https://upload-images.jianshu.io/upload_images/11576306-6c680bc288fcaa42.png?imageMogr2/auto-orient/strip|imageView2/2/w/425/format/webp)

image

假设随机森林共有n棵树，那么：

![img](https://upload-images.jianshu.io/upload_images/11576306-c96568040a7f0381.png?imageMogr2/auto-orient/strip|imageView2/2/w/355/format/webp)

最后把所有求得的重要性评分进行归一化处理就得到重要性的评分：

![img](https://upload-images.jianshu.io/upload_images/11576306-399e016fa95d18ba.png?imageMogr2/auto-orient/strip|imageView2/2/w/304/format/webp)

通过sklearn中的随机森林返回特征的重要性：

值得庆幸的是，sklearnsklearn已经帮我们封装好了一切，我们只需要调用其中的函数即可。
我们以UCI上葡萄酒的例子为例，首先导入数据集。

然后，我们来大致看下这时一个怎么样的数据集

输出为

可见共有3个类别。然后再来看下数据的信息：

输出为

可见除去class label之外共有13个特征，数据集的大小为178。
按照常规做法，将数据集分为训练集和测试集。

好了，这样一来随机森林就训练好了，其中已经把特征的重要性评估也做好了，我们拿出来看下。

输出的结果为

对的就是这么方便。
如果要筛选出重要性比较高的变量的话，这么做就可以

输出为

瞧，这不，帮我们选好了3个重要性大于0.15的特征了吗~

# 参考文献

[1] Raschka S. Python Machine Learning[M]. Packt Publishing, 2015.
[2] 杨凯, 侯艳, 李康. 随机森林变量重要性评分及其研究进展[J]. 2015.
