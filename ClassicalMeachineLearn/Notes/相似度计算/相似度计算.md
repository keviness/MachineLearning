# 推荐算法入门（1）相似度计算方法大全

# **一.协同过滤算法简介**

在推荐系统的众多方法之中，基于用户的协同过滤是诞最早的，原理也比较简单。基于协同过滤的推荐算法被广泛的运用在推荐系统中，比如影视推荐、猜你喜欢等、邮件过滤等。该算法1992年提出并用于邮件过滤系统，两年后1994年被 GroupLens 用于新闻过滤。一直到2000年，该算法都是推荐系统领域最著名的算法。

当用户A需要个性化推荐的时候，可以 **先找到和他兴趣详细的用户集群G** ，然后把 **G喜欢的并且A没有的物品推荐给A** ，这就是基于用户的协同过滤。

根据上述原理，我们可以将算法分为两个步骤：

1. 找到与目标兴趣**相似**的用户集群
2. 找到这个集合中用户喜欢的、并且目标用户没有听说过的物品推荐给目标用户。

## **二、常用的相似度计算方法**

下面，简单的举例几个机器学习中常用的样本相似性度量方法：

1. 欧式距离（Euclidean Distance）
2. 余弦相似度（Cosine）
3. 皮尔逊相关系数（Pearson）
4. 修正余弦相似度（Adjusted Cosine）
5. 汉明距离（Hamming Distance）
6. 曼哈顿距离（Manhattan Distance）

## **1、 欧式距离（Euclidean Distance）**

欧式距离全称是欧几里距离，是最易于理解的一种距离计算方式，源自欧式空间中两点间的距离公式。

1. 平面空间内的 ![[公式]](https://www.zhihu.com/equation?tex=a%28x1%2Cy1%29) 与 ![[公式]](https://www.zhihu.com/equation?tex=b%28x2%2Cy2+%29) 间的欧式距离：

![[公式]](https://www.zhihu.com/equation?tex=d+%3D+%5Csqrt%7B%28x1+-+x2%29%5E2+%2B+%28y1-y2%29%5E2%7D)

2. 三维空间里的欧式距离：

![[公式]](https://www.zhihu.com/equation?tex=d+%3D+%5Csqrt%7B%28x1+-+x2%29%5E2+%2B+%28y1-y2%29%5E2%2B%28z1-z2%29%5E2%7D)

3. Python 代码简单实现：

```python
def EuclideanDistance(x,y):
	d = 0
	for a,b in zip(x,y):
		d += (a-b)**2
	return d**0.5
```

4. 使用 numpy 简化：

```python
import numpy as np
def EuclideanDistance(dataA,dataB):
    # np.linalg.norm 用于范数计算，默认是二范数，相当于平方和开根号
    return 1.0/(1.0 + np.linalg.norm(dataA - dataB))
```

## **2、余弦相似度（Cosine）**

首先，样本数据的夹角余弦并不是真正几何意义上的夹角余弦，只不过是借了它的名字，实际是借用了它的概念变成了是代数意义上的“夹角余弦”，用来衡量样本向量间的差异。

![](https://pic2.zhimg.com/80/v2-2a8cf3b980fd6f4ecafd424401819519_1440w.jpg)
几何意义上的夹角余弦

夹角越小，余弦值越接近于1，反之则趋于-1。我们假设有x1与x2两个向量：

![[公式]](https://www.zhihu.com/equation?tex=cos%28%5Ctheta%29+%3D+%5Cfrac%7B%5Csum_%7Bk%3D1%7D%5E%7Bn%7D%7Bx_%7B1k%7D%7Dx_%7B2k%7D%7D%7B%5Csqrt%7B%5Csum_%7Bk%3D1%7D%5E%7Bn%7D%7Bx_%7B1k%7D%7D%5E2%7D%5Csqrt%7B%5Csum_%7Bk%3D1%7D%5E%7Bn%7D%7Bx_%7B2k%7D%7D%5E2%7D%7D)

1. Python 代码的简单按公式还原：

```python
def Cosine(x,y):
    sum_xy = 0.0;  
    normX = 0.0;  
    normY = 0.0;  
    for a,b in zip(x,y):  
        sum_xy += a*b  
        normX += a**2  
        normY += b**2  
    if normX == 0.0 or normY == 0.0:  
        return None  
    else:  
        return sum_xy / ((normX*normY)**0.5)  
```

2. 使用 numpy 简化夹角余弦

```python
def Cosine(dataA,dataB):
    sumData = dataA *dataB.T # 若列为向量则为 dataA.T * dataB
    denom = np.linalg.norm(dataA) * np.linalg.norm(dataB)
    # 归一化
    return 0.5 + 0.5 * (sumData / denom)
```

我们引入一组特殊数据进行测试：

```python
dataA = np.mat([1,2,3,3,2,1])
dataB = np.mat([2,3,4,4,3,2])
print(EuclideanDistance(dataA,dataB)) # 0.28
print(Cosine(dataA,dataB)) # 0.99
```

**欧式距离和夹角余弦的区别：**

对比以上的结果的 dataA 与 dataB 这两组数据，会发现 dataA 与 dataB 的欧式距离相似度比较小，而夹角余弦相似度比较大，即夹角余弦更能 **反映两者之间的变动趋势** ，两者有很高的 **变化趋势相似度** ，而欧式距离较大是因为 **两者数值有很大的区别** ，即两者拥有很高的 **数值差异** 。

## **3、皮尔逊相关系数（Pearson Correlation Coefficient）**

[如何理解皮尔逊相关系数（Pearson Correlation Coefficient）？**1404 关注 · 21 回答**问题](https://www.zhihu.com/question/19734616)

假如之不先介绍夹角余弦的话，第一次接触你绝对会对皮尔逊相关系数一脸懵逼。那么现在，让我们再来理解一下皮尔逊相关系数的公式：

![[公式]](https://www.zhihu.com/equation?tex=sim%28x_1%2Cx_2%29+%3D+%5Cfrac%7B%5Csum_%7Bk%3D1%7D%5E%7Bn%7D%7B%28x_%7B1k%7D+-+%5Cbar%7Bx_1%7D%29%7D%28x_%7B2k%7D+-+%5Cbar%7Bx_2%7D%29%7D%7B%5Csqrt%7B%5Csum_%7Bk%3D1%7D%5E%7Bn%7D%7B%28x_%7B1k%7D+-+%5Cbar%7Bx_1%7D%29%7D%5E2%7D%5Csqrt%7B%5Csum_%7Bk%3D1%7D%5E%7Bn%7D%7B%28x_%7B2k%7D+-+%5Cbar%7Bx_2%7D%29%7D%5E2%7D%7D)

* 这里减去的 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbar%7Bx_2%7D) 每个item被打分的均值

皮尔逊相关系数公式实际上就是在计算夹角余弦之前将两个向量 **减去各个样本的平均值** ，达到**中心化**的目的。从知友的回答可以明白，皮尔逊相关函数是 **余弦相似度在维度缺失上面的一种改进方法** 。

1.Python 代码实现皮尔逊相关系数：

```python
def Pearson(x,y):
    sum_XY = 0.0
    sum_X = 0.0
    sum_Y = 0.0
    normX = 0.0
    normY = 0.0
    count = 0
    for a,b in zip(x,y):
        count += 1
        sum_XY += a * b
        sum_X += a
        sum_Y += b
        normX += a**2
        normY += b**2
    if count == 0:
        return 0
    # denominator part
    denominator = (normX - sum_X**2 / count)**0.5 * (normY - sum_Y**2 / count)**0.5
    if denominator == 0:
        return 0
    return (sum_XY - (sum_X * sum_Y) / count) / denominator
```

2. numpy 简化实现皮尔逊相关系数

```python
def Pearson(dataA,dataB):
    # 皮尔逊相关系数的取值范围(-1 ~ 1),0.5 + 0.5 * result 归一化(0 ~ 1)
    return 0.5 + 0.5 * np.corrcoef(dataA,dataB,rowvar = 0)[0][1]
```

**用余弦相似度相同的方法实现皮尔逊：**

```python
# 余弦相似度、修正余弦相似度、皮尔逊相关系数的关系
# Pearson 减去的是每个item i 的被打分的均值
def Pearson(dataA,dataB):
    avgA = np.mean(dataA)
    avgB = np.mean(dataB)
    sumData = (dataA - avgA) * (dataB - avgB).T # 若列为向量则为 dataA.T * dataB
    denom = np.linalg.norm(dataA - avgA) * np.linalg.norm(dataB - avgB)
    # 归一化
    return 0.5 + 0.5 * (sumData / denom)
```

## **4、修正余弦相似度**

1. 为什么需要在余弦相似度的基础上使用修正余弦相似度

> X和Y两个用户对两个内容的评分分别为（1,2）和（4,5），使用余弦相似度得到的结果是0.98，两者极为相似。但从评分上看X似乎不喜欢2这个 内容，而Y则比较喜欢，余弦相似度对数值的不敏感导致了结果的误差，需要修正这种不合理性

```python
# 修正余弦相似度
# 修正cosine 减去的是对item i打过分的每个user u，其打分的均值
data = np.mat([[1,2,3],[3,4,5]])
avg = np.mean(data[:,0]) # 下标0表示正在打分的用户
def AdjustedCosine(dataA,dataB,avg):
    sumData = (dataA - avg) * (dataB - avg).T # 若列为向量则为 dataA.T * dataB
    denom = np.linalg.norm(dataA - avg) * np.linalg.norm(dataB - avg)
    return 0.5 + 0.5 * (sumData / denom)
print(AdjustedCosine(data[0,:],data[1,:],avg))
```

## **5、汉明距离（Hamming distance）**

汉明距离表示的是两个字符串（相同长度） **对应位不同的数量** 。比如有两个等长的字符串 str1 = "11111" 和 str2 = "10001" 那么它们之间的汉明距离就是3（这样说就简单多了吧。哈哈）。汉明距离多用于图像像素的匹配（同图搜索）。

1.Python 的矩阵汉明距离简单运用：

```text
def hammingDistance(dataA,dataB):
    distanceArr = dataA - dataB
    return np.sum(distanceArr == 0)# 若列为向量则为 shape[0]
```

## **6.曼哈顿距离（Manhattan Distance）**

没错，你也是会曼哈顿计量法的人了，现在开始你和秦风只差一张刘昊然的脸了。想象你在曼哈顿要从一个十字路口开车到另外一个十字路口，那么驾驶的最近距离并不是直线距离，因为你不可能横穿房屋。所以，曼哈顿距离表示的就是你的实际驾驶距离，即两个点在标准坐标系上的绝对轴距总和。

![](https://pic2.zhimg.com/80/v2-f3abdee3ea4c913aa10041f6cd56ccdd_1440w.jpg)
曼哈顿距离

```python
# 曼哈顿距离(Manhattan Distance)
def Manhattan(dataA,dataB):
    return np.sum(np.abs(dataA - dataB))
print(Manhattan(dataA,dataB))
```

## **二、了解数据结构**

**以下题目和数据均来自于[千里码](https://link.zhihu.com/?target=http%3A//www.qlcoder.com/)，一个优质的程序员答题网站，由于站长食年常年失踪，于是我就无耻的分享出来啦。**

> 现在从豆瓣的用户中抽取了500左右个比较活跃的用户，这些用户都是忠实的电影迷，大部分人涉猎了上百部电影。
>
> 这里有个80多万行的[文本文件](https://link.zhihu.com/?target=http%3A//www.qlcoder.com/download/train.txt)，文件的每行是三个数字，分别是 userid，movieid，rating。代表一个用户对一部电影的评分。rating代表评分的星级，如上图中的红框所示，星级从低到高依次是1-5。
>
> 接下来有个行数为10001的[文本文件](https://link.zhihu.com/?target=http%3A//www.qlcoder.com/download/test.txt)（第一行为title），文件的每行为2个数字，分别代表userid和movieid，请你预测如果该用户观看了这部电影，会给该电影打多少分，你的预测值为1个大小为1-5的整数。
>
> 本题的答案是一个长度为1万的字符串，字符串的第k位代表你对第k行的预测结果。
> 如果你的预测结果和实际答案的差值的绝对值的和小于6000，通过该题。
>
