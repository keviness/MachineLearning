# 短文本相似度计算基础模型

文本相似性是文本匹配中的一个主要任务。文本的相似性其实是一个比较模糊的概念，需要结合现实的场景做准确区分。相似与相关不是相同的概念，而文本相似从细分上来说主要是： **文本层面相似（表面）和语义相似（内涵）** 。该文主要讲的是文本层面的相似。（无监督模型）

## **jaccard**

 **jaccard距离也称为杰卡德距离，** 一般用来度量两个集合间的相似度大小。假设两个结合为A和B，那么两者的jaccard相似度计算式为：

![[公式]](https://www.zhihu.com/equation?tex=jaccard%3D%5Cfrac%7B%7CA%5Ccap+B%7C%7D%7B%7CA%5Ccup+B%7C%7D%3D%5Cfrac%7Blen%28A%5Ccap+B%29%7D%7Blen%28A%5Ccup+B%29%7D)

jaccrad距离的思想很简单，两个集合共有的元素越多，二者越相似。

## cqr&ctr

cqr和ctr的概念还是简单的，假设给定query ![[公式]](https://www.zhihu.com/equation?tex=Q%3Dq_%7B1%7D%2Cq_%7B2%7D%2C...%2Cq_%7Bm%7D) 和title ![[公式]](https://www.zhihu.com/equation?tex=T%3Dt_%7B1%7D%2Ct_%7B2%7D%2C...%2Ct_%7Bn%7D) ,那么cqr和ctr的计算式分别为：

![[公式]](https://www.zhihu.com/equation?tex=cqr%3D%5Cfrac%7B%7CQ%5Ccap+T%7C%7D%7B%7CQ%7C%7D%3D%5Cfrac%7Blen%28Q%5Ccap+T%29%7D%7Blen%28Q%29%7D)

![[公式]](https://www.zhihu.com/equation?tex=ctr%3D%5Cfrac%7B%7CQ%5Ccap+T%7C%7D%7B%7CT%7C%7D%3D%5Cfrac%7Blen%28Q%5Ccap+T%29%7D%7Blen%28T%29%7D)

即表示二者的交集分别占query和title的比例是多少，占比越大，相似度也越大。

当然，由于这种计算 **“均等化”** 所有词的重要性（词权重），例如“怎么做NLP【怎么/做/NLP】”分别和“怎样做NLP【怎样/做/NLP】”、“怎么做TNT”【怎么/做/TNT】进行计算，两者相似度均为 ![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B2%7D%7B3%7D) ，这不是完完全全的大笑话了嘛，因此就得考虑到给每个词分配不同的权重，这就是一个优化版本。

对于给定的query ![[公式]](https://www.zhihu.com/equation?tex=Q%3Dq_%7B1%7D%2Cq_%7B2%7D%2C...%2Cq_%7Bm%7D) ，对应的权重是 ![[公式]](https://www.zhihu.com/equation?tex=W_%7Bq%7D%3Dw_%7Bq_%7B1%7D%7D%2Cw_%7Bq_%7B2%7D%7D%2C...%2Cw_%7Bq_%7Bm%7D%7D) 和title ![[公式]](https://www.zhihu.com/equation?tex=T%3Dt_%7B1%7D%2Ct_%7B2%7D%2C...%2Ct_%7Bn%7D) ,对应的权重是 ![[公式]](https://www.zhihu.com/equation?tex=W_%7Bt%7D%3Dw_%7Bt_%7B1%7D%7D%2Cw_%7Bt_%7B2%7D%7D%2C...%2Cw_%7Bt_%7Bn%7D%7D) ,那么经优化的cqr和ctr计算式为：

![[公式]](https://www.zhihu.com/equation?tex=cqr_%7Bopt%7D%3D%5Cfrac%7B%5Csum_%7Bi%5Cepsilon+Q%5Ccap+T%7D%5E%7B%7D%7Bw_%7Bi%7D%7D%7D%7B%5Csum_%7Bj%5Cepsilon+Q%7D%5E%7B%7D%7Bw_%7Bj%7D%7D%7D)

![[公式]](https://www.zhihu.com/equation?tex=ctr_%7Bopt%7D%3D%5Cfrac%7B%5Csum_%7Bi%5Cepsilon+Q%5Ccap+T%7D%5E%7B%7D%7Bw_%7Bi%7D%7D%7D%7B%5Csum_%7Bj%5Cepsilon+T%7D%5E%7B%7D%7Bw_%7Bj%7D%7D%7D)

若是要分析query和title间综合、无偏的相似度，则通过cqr和ctr相乘即可：

![[公式]](https://www.zhihu.com/equation?tex=sim+%3D+cqr%5Cast+ctr)

通过以上的计算式可知，jaccard与cqr&ctr的区别与联系。cqr和ctr相当于是jaccard的拆分。

当query和title长度计算差距很大的时候，jaccard计算准确性就会受到影响，而分成两个指标cqr和ctr，则能够充分表现两者的相似性。

例如：query是“我昨天新买的手机，今天怎么就不能开机了”，title是“手机不能开机”，这里，ctr无疑就是1，当然cqr就比较低， **但是我们可以用ctr作为后续的排序特征或者过滤条件** 。

## **余弦相似度**

余弦相似度，就是用空间中两个向量(a, b)的夹角，来判断这两个向量的相似程度。

计算式： ![[公式]](https://www.zhihu.com/equation?tex=cos%5Ctheta%3D%5Cfrac%7Ba%5Ccdot+b%7D%7B%7C%7Ca%7C%7C%5Ctimes%7C%7Cb%7C%7C%7D)

使用BOW作为文本的特征提取时，当词袋很大时，即词向量表示过大，因此就需要寻求一种有效的**特征降维**方法，能够在不损伤核心信息的情况下降低向量空间的维数，比如**TF-IDF**算法。

## **TF-IDF**

**简单概念：**

TF-IDF（term frequency–inverse document frequency，词频-逆向文件频率）是一种用于信息检索（information retrieval）与文本挖掘（text mining）的常用 **加权技术** 。

TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。**字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。**

 **TF-IDF主要思想** ：如果某个单词在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则 **该词汇越能够表示该文本的信息** ，认为该词汇具有很好的类别区分能力，适合用来分类。

**TF-IDF相似度求解步骤：**

1. 先对query和doc文本进行分词及去停用词操作；
2. 每篇文章各取出若干个词合并成一个集合，计算每篇文章对于这个集合中词的词频（为了避免文章长度的差异，可以使用相对词频）及逆文档频率；
3. 计算query和doc之间的tf-idf累加得分，值越大就表示越相似。

 **TF-IDF文本相似度度量示例代码** ：

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import numpy as np

class TFIDF_Sim(object):
    def __init__(self, docs_lst):
        self.docs_lst = docs_lst  # word cutted
        self.docs_number = len(docs_lst)
        self.tf = []
        self.idf = {}
        self.init()

    def init(self):
        df = {}
        for document in self.docs_lst:
            temp = {}
            for word in document:
                temp[word] = temp.get(word, 0) + 1/len(document)
            self.tf.append(temp)
            for key in temp.keys():
                df[key] = df.get(key, 0) + 1
        for key, value in df.items():
            self.idf[key] = np.log(self.docs_number / (value + 1))

    def get_score(self, index, query):
        score = 0.0
        for q in query:
            if q not in self.tf[index]:
                continue
            score += self.tf[index][q] * self.idf[q]
        return score

    def get_docs_score(self, query):
        score_list = []
        for i in range(self.docs_number):
            score_list.append(self.get_score(i, query))
        return score_list


if __name__ == "__main__":
    import jieba

    document_list = ["行政机关强行解除行政协议造成损失，如何索取赔偿？",
                     "借钱给朋友到期不还得什么时候可以起诉？怎么起诉？",
                     "我在微信上被骗了，请问被骗多少钱才可以立案？",
                     "公民对于选举委员会对选民的资格申诉的处理决定不服，能不能去法院起诉吗？",
                     "有人走私两万元，怎么处置他？",
                     "法律上餐具、饮具集中消毒服务单位的责任是不是对消毒餐具、饮具进行检验？"]
    doc_list = [list(jieba.cut(doc)) for doc in document_list]
    tf_idf_model = TFIDF_Sim(doc_list)
    print(f"document_list:{tf_idf_model.docs_lst}")
    print(f"document_number:{tf_idf_model.docs_number}")
    print(f"TF:{tf_idf_model.tf}")
    print(f"IDF:{tf_idf_model.idf}")
    query = "走私了两万元，在法律上应该怎么量刑？"
    query_cut = list(jieba.cut(query))
    scores = tf_idf_model.get_docs_score(query_cut)

    print(f"document_scores:{scores}")
    print(f"The most similarity to {query} is {document_list[np.argmax(scores)]}")
```

## BM25

BM25算法，其中BM表示Best Match，它是TF-IDF算法的优化版本。

BM25算法是一种计算句子query与文档doc相关性的算法，它的原理十分简单：将输入的句子query进行 **分词** ，然后分别 **计算句子中每个词word与文档doc的相关度** ，然后进行加权求和，得到句子与文档的相关度评分。评分公式如下：

![[公式]](https://www.zhihu.com/equation?tex=Score%28Q%2CD%29+%3D+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%7BW_%7Bi%7DR%28q_%7Bi%7D%2CD%29%7D)

上面公式中 ![[公式]](https://www.zhihu.com/equation?tex=W_%7Bi%7D) 表示权重，也就 **相当于IDF值** 。![[公式]](https://www.zhihu.com/equation?tex=Q)是query， ![[公式]](https://www.zhihu.com/equation?tex=D)表示doc，n为Q中的单词数，![[公式]](https://www.zhihu.com/equation?tex=R%28q_%7Bi%7D%2CD%29) 是word q与文档D的相关性得分，相当于TF值。

![[公式]](https://www.zhihu.com/equation?tex=W_%7Bi%7D) 和 ![[公式]](https://www.zhihu.com/equation?tex=R%28q_%7Bi%7D%2CD%29) 的计算式如下：

![[公式]](https://www.zhihu.com/equation?tex=W_%7Bi%7D%3Dlog%28%5Cfrac%7BN-df_%7Bi%7D%2B0.5%7D%7Bdf_%7Bi%7D%2B0.5%7D%29)

其中N表示doc的总数， ![[公式]](https://www.zhihu.com/equation?tex=df_%7Bi%7D) 表示包含词汇 ![[公式]](https://www.zhihu.com/equation?tex=q_%7Bi%7D) 的doc的数量。

![[公式]](https://www.zhihu.com/equation?tex=R%28q_%7Bi%7D%2CD%29%3D%5Cfrac%7Bf_%7Bi%7D%28k_%7B1%7D%2B1%29%7D%7Bf_%7Bi%7D%2BK%7D%5Cast+%5Cfrac%7Bqf_%7Bi%7D%28k_%7B2%7D%2B1%29%7D%7Bqf_%7Bi%7D%2Bk_%7B2%7D%7D)

![[公式]](https://www.zhihu.com/equation?tex=K%3Dk_%7B1%7D%2A%281-b%2Bb%2A%5Cfrac%7Bdl%7D%7Bdl_%7Bavg%7D%7D%29)

其中， ![[公式]](https://www.zhihu.com/equation?tex=k_%7B1%7D%2Ck_%7B2%7D) 和 ![[公式]](https://www.zhihu.com/equation?tex=b) 是超参数，一般分别取值为2，1，0.75； ![[公式]](https://www.zhihu.com/equation?tex=f_%7Bi%7D) 表示词汇 ![[公式]](https://www.zhihu.com/equation?tex=q_%7Bi%7D) 在D中出现的次数； ![[公式]](https://www.zhihu.com/equation?tex=qf_%7Bi%7D) 表示词汇 ![[公式]](https://www.zhihu.com/equation?tex=q_%7Bi%7D) 在Q中出现的次数； ![[公式]](https://www.zhihu.com/equation?tex=dl) 为doc的文本长度； ![[公式]](https://www.zhihu.com/equation?tex=dl_%7Bavg%7D) 表示D中所有doc的平均文本长度。

 **bm25文本相似度度量示例代码** ：

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import numpy as np
from collections import Counter

class BM25_Sim(object):
    def __init__(self, docs_lst, k1=2, k2=1, b=0.75):
        self.docs_lst = docs_lst
        self.docs_number = len(docs_lst)
        self.avg_docs_len = sum([len(document) for document in docs_lst]) / self.docs_number
        self.tf = []
        self.idf = {}
        self.k1 = k1
        self.k2 = k2
        self.b = b
        self.init()

    def init(self):
        df = {}
        for document in self.docs_lst:
            temp = {}
            for word in document:
                temp[word] = temp.get(word, 0) + 1
            self.tf.append(temp)
            for key in temp.keys():
                df[key] = df.get(key, 0) + 1
        for key, value in df.items():
            self.idf[key] = np.log((self.docs_number - value + 0.5) / (value + 0.5))

    def get_score(self, index, query):
        score = 0.0
        document_len = len(self.tf[index])
        qf = Counter(query)
        for q in query:
            if q not in self.tf[index]:
                continue
            score += self.idf[q] * (self.tf[index][q] * (self.k1 + 1) / (
                        self.tf[index][q] + self.k1 * (1 - self.b + self.b * document_len / self.avg_docs_len))) * (
                                 qf[q] * (self.k2 + 1) / (qf[q] + self.k2))

        return score

    def get_docs_score(self, query):
        score_list = []
        for i in range(self.docs_number):
            score_list.append(self.get_score(i, query))
        return score_list


if __name__ == "__main__":
    import jieba

    document_list = ["行政机关强行解除行政协议造成损失，如何索取赔偿？",
                     "借钱给朋友到期不还得什么时候可以起诉？怎么起诉？",
                     "我在微信上被骗了，请问被骗多少钱才可以立案？",
                     "公民对于选举委员会对选民的资格申诉的处理决定不服，能不能去法院起诉吗？",
                     "有人走私两万元，怎么处置他？",
                     "法律上餐具、饮具集中消毒服务单位的责任是不是对消毒餐具、饮具进行检验？"]
    doc_list = [list(jieba.cut(doc)) for doc in document_list]
    bm25_sim = BM25_Sim(doc_list)
    print(f"document_list:{bm25_sim.docs_lst}")
    print(f"document_number:{bm25_sim.docs_number}")
    print(f"TF:{bm25_sim.tf}")
    print(f"IDF:{bm25_sim.idf}")
    query = "走私了两万元，在法律上应该怎么量刑？"
    query_cut = list(jieba.cut(query))
    scores = bm25_sim.get_docs_score(query_cut)

    print(f"document_scores:{scores}")
    print(f"The most similarity to {query} is {document_list[np.argmax(scores)]}")
```
