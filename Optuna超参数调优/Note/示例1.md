> ğŸ”— åŸæ–‡é“¾æ¥ï¼š [https://blog.csdn.net/qq_43510916/a...](https://blog.csdn.net/qq_43510916/article/details/113794486)

> â° å‰ªå­˜æ—¶é—´ï¼š2023-03-12 16:30:09 (UTC+8)

> âœ‚ï¸ æœ¬æ–‡æ¡£ç”± [é£ä¹¦å‰ªå­˜ ](https://www.feishu.cn/hc/zh-CN/articles/606278856233?from=in_ccm_clip_doc)ä¸€é”®ç”Ÿæˆ

# ã€æœºå™¨å­¦ä¹ ã€‘Optunaæœºå™¨å­¦ä¹ æ¨¡å‹è°ƒå‚(LightGBMã€XGBoost)

### æ–‡ç« ç›®å½•

* 1. optunaç®€ä»‹
* 2. LGBMå’ŒXGBoostè°ƒå‚æ±‡æ€»

  * 2.1 LGBM
    * 2.1.1 å®šä¹‰Objective
    * 2.1.2 è°ƒå‚try
    * 2.1.3 ç»˜å›¾
    * 2.1.4 æœ€ä½³å‚æ•°
  * 2.2 XGBOOST
    * 2.2.1 å®šä¹‰Objectove
    * 2.2.2 è°ƒå‚try
    * 2.2.3 ç»˜å›¾
    * 2.2.4 æœ€ä½³å‚æ•°

# 1. optunaç®€ä»‹

åœ¨ [Kaggle ](https://so.csdn.net/so/search?q=Kaggle&spm=1001.2101.3001.7020)æ¯”èµ›çš„è¿‡ç¨‹ä¸­æˆ‘å‘ç°äº†ä¸€ä¸ªé—®é¢˜ï¼ˆå¤§å®¶çš„Kernelæ¨¡å‹ä¸­åŒ…å«äº†ä¼—å¤šcè¶…å‚æ•°è®¾ç½®ï¼Œä½†æ˜¯è¿™äº›å‚æ•°æ˜¯å¦‚ä½•è®¾ç½®çš„å‘¢ï¼Ÿï¼‰ï¼Œå¹¶åœ¨Discussionä¸­æå‡ºäº†æˆ‘çš„é—®é¢˜ï¼Œå¹¶å¾—åˆ°äº†ä¼—å¤šå¤§ä½¬çš„å›ç­”ï¼Œå¦‚ä¸‹ï¼š

![](https://fjjwhjwd3p.feishu.cn/space/api/box/stream/download/asynccode/?code=ZjgzMmQ4ZWIwMzQ5ODEzNjk4ODMwZGM2ZjY1NjAwM2NfQlpIY3hoTEhYdFpDSmVvRnBrVUI2d2k1cW5BT0w5Q0xfVG9rZW46SjNRZGJNYXFhb2x1V0V4OXFYZ2NSVHdGbmRkXzE2Nzg2MDk4MjE6MTY3ODYxMzQyMV9WNA)

å…³äºå›ç­”æˆ‘æ±‡æ€»åå‘ç°éƒ½æåˆ°äº†å…³äºoptunaåº“çš„ä½¿ç”¨ï¼Œoptunaæ˜¯ä»€ä¹ˆå‘¢ï¼Ÿoptunaæ˜¯ä¸€ä¸ªä½¿ç”¨pythonç¼–å†™çš„è¶…å‚æ•°è°ƒèŠ‚æ¡†æ¶ã€‚ä¸€ä¸ªæç®€çš„ optuna çš„ä¼˜åŒ–ç¨‹åºä¸­åªæœ‰ä¸‰ä¸ªæœ€æ ¸å¿ƒçš„æ¦‚å¿µï¼Œç›®æ ‡å‡½æ•°(objective)ï¼Œå•æ¬¡è¯•éªŒ(trial)ï¼Œå’Œç ”ç©¶(study). å…¶ä¸­ objective è´Ÿè´£å®šä¹‰å¾…ä¼˜åŒ–å‡½æ•°å¹¶æŒ‡å®šå‚/è¶…å‚æ•°æ•°èŒƒå›´ï¼Œtrial å¯¹åº”ç€ objective çš„å•æ¬¡æ‰§è¡Œï¼Œè€Œ study åˆ™è´Ÿè´£ç®¡ç†ä¼˜åŒ–ï¼Œå†³å®šä¼˜åŒ–çš„æ–¹å¼ï¼Œæ€»è¯•éªŒçš„æ¬¡æ•°ã€è¯•éªŒç»“æœçš„è®°å½•ç­‰åŠŸèƒ½ã€‚
ä¸‹é¢ä¸¾ä¸€ä¸ªç®€å•çš„æ —å­ï¼Œæœ‰åŠ©äºå¤§å®¶çš„ç†è§£ï¼š

> å®šä¹‰ x , y âˆˆ ( âˆ’ 10 , 10 ) x,y\in(-10, 10)  x ,  y  âˆˆ   ( âˆ’ 1 0 ,  1 0 ) ï¼Œæ±‚ f ( x ) = ( x + y ) 2 f(x)=(x+y)^2  f ( x )  =   ( x  +   y )  2 å–å¾—æœ€å¤§å€¼æ—¶ï¼Œ x , y x,y  x ,  y çš„å–å€¼ï¼Ÿ

```Python
import optuna
 
def objective(trial):
    x = trial.suggest_uniform('x', -10, 10)
    y = trial.suggest_uniform('y', -10, 10)
    return (x + y) ** 2
 
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)
 
print(study.best_params)
print(study.best_value)
```

# 2. LGBMå’Œ [XGBoost ](https://so.csdn.net/so/search?q=XGBoost&spm=1001.2101.3001.7020)è°ƒå‚æ±‡æ€»

## 2.1 LGBM

### 2.1.1 å®šä¹‰Objective

```Python
from lightgbm import LGBMRegressor
import optuna
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split, KFold
import optuna.integration.lightgbm as oplgb

def objective(trial):
    X_train, X_test, y_train, y_test=train_test_split(data, target, train_size=0.3)# æ•°æ®é›†åˆ’åˆ†
    param = {
        'metric': 'rmse', 
        'random_state': 48,
        'n_estimators': 20000,
        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),
        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),
        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),
        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),
        'learning_rate': trial.suggest_categorical('learning_rate', [0.006,0.008,0.01,0.014,0.017,0.02]),
        'max_depth': trial.suggest_categorical('max_depth', [5, 7, 9, 11, 13, 15, 17, 20, 50]),
        'num_leaves' : trial.suggest_int('num_leaves', 1, 1000),
        'min_child_samples': trial.suggest_int('min_child_samples', 1, 300),
        'cat_smooth' : trial.suggest_int('cat_smooth', 1, 100)    
    }
  
    lgb=LGBMRegressor(**param)
    lgb.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=100, verbose=False)
    pred_lgb=lgb.predict(X_test)
    rmse = mean_squared_error(y_test, pred_lgb, squared=False)
    return rmse
```

### 2.1.2 è°ƒå‚try

```Python
study=optuna.create_study(direction='minimize')
n_trials=50 # try50æ¬¡
study.optimize(objective, n_trials=n_trials)
```

### 2.1.3 ç»˜å›¾

```Python
optuna.visualization.plot_optimization_history(study)# ç»˜åˆ¶
```

```Python
optuna.visualization.plot_parallel_coordinate(study)# 
```

```Python
optuna.visualization.plot_param_importances(study)# 
```

### 2.1.4 æœ€ä½³å‚æ•°

```Python
params=study.best_params
params['metric'] = 'rmse'
```

## 2.2 XGBOOST

### 2.2.1 å®šä¹‰Objectove

```Python
def objective(trial):
    data = train.iloc[:, :-1]
    target = train.target
    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.3, random_state=42)
    param = {
        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),
        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),
        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]),
        'subsample': trial.suggest_categorical('subsample', [0.4, 0.5, 0.6, 0.7, 0.8, 1.0]),
        'learning_rate': trial.suggest_categorical('learning_rate',
                                                   [0.008, 0.009, 0.01, 0.012, 0.014, 0.016, 0.018, 0.02]),
        'n_estimators': 4000,
        'max_depth': trial.suggest_categorical('max_depth', [5, 7, 9, 11, 13, 15, 17, 20]),
        'random_state': trial.suggest_categorical('random_state', [24, 48, 2020]),
        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),
    }
    model = xgb.XGBRegressor(**param)
    model.fit(train_x, train_y, eval_set=[(test_x, test_y)], early_stopping_rounds=100, verbose=False)
    preds = model.predict(test_x)
    rmse = mean_squared_error(test_y, preds, squared=False)
    return rmse
```

### 2.2.2 è°ƒå‚try

```Python
study = optuna.create_study(direction='minimize')
n_trials=1
study.optimize(objective, n_trials=n_trials)
print('Number of finished trials:', len(study.trials))
print("------------------------------------------------")
print('Best trial:', study.best_trial.params)
print("------------------------------------------------")
print(study.trials_dataframe())
print("------------------------------------------------")
```

### 2.2.3 ç»˜å›¾

```Python
optuna.visualization.plot_optimization_history(study).show()
#plot_parallel_coordinate: interactively visualizes the hyperparameters and scores
optuna.visualization.plot_parallel_coordinate(study).show()
'''plot_slice: shows the evolution of the search. You can see where in the hyperparameter space your search
went and which parts of the space were explored more.'''
optuna.visualization.plot_slice(study).show()
optuna.visualization.plot_contour(study, params=['alpha',
                            #'max_depth',
                            'lambda',
                            'subsample',
                            'learning_rate',
                            'subsample']).show()
#Visualize parameter importances.
optuna.visualization.plot_param_importances(study).show()
#Visualize empirical distribution function
optuna.visualization.plot_edf(study).show()
```

### 2.2.4 æœ€ä½³å‚æ•°

```Python
params=study.best_params
```
